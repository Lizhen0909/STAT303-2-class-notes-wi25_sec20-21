[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science II with python (Class notes)",
    "section": "",
    "text": "Preface\nThese are class notes for the course STAT303-2. This is not the course text-book. You are required to read the relevant sections of the book as mentioned on the course website.\nThe course notes are currently being written, and will continue to being developed as the course progresses (just like the course textbook last quarter). Please report any typos / mistakes / inconsistencies / issues with the class notes / class presentations in your comments here. Thank you!"
  },
  {
    "objectID": "Lec1_SimpleLinearRegression.html",
    "href": "Lec1_SimpleLinearRegression.html",
    "title": "1  Simple Linear Regression",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nDevelop a simple linear regression model that predicts car price based on engine size. Datasets to be used: Car_features_train.csv, Car_prices_train.csv\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990\n    \n  \n\n\n\n\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nols_object = smf.ols(formula = 'price~engineSize', data = train)\n\n\n#Using the fit() function of the 'ols' class to fit the model\nmodel = ols_object.fit()\n\n\n#Printing model summary which contains among other things, the model coefficients\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.390 \n\n\n  Model:                   OLS         Adj. R-squared:        0.390 \n\n\n  Method:             Least Squares    F-statistic:           3177. \n\n\n  Date:             Thu, 19 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 16:44:04       Log-Likelihood:      -53949. \n\n\n  No. Observations:        4960        AIC:                1.079e+05\n\n\n  Df Residuals:            4958        BIC:                1.079e+05\n\n\n  Df Model:                   1                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -4122.0357   522.260    -7.893  0.000 -5145.896 -3098.176\n\n\n  engineSize  1.299e+04   230.450    56.361  0.000  1.25e+04  1.34e+04\n\n\n\n\n  Omnibus:       1271.986   Durbin-Watson:         0.517\n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   6490.719\n\n\n  Skew:            1.137    Prob(JB):               0.00\n\n\n  Kurtosis:        8.122    Cond. No.               7.64\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe model equation is: car price = -4122.0357 + 12990 * engineSize\nVisualize the regression line\n\nsns.regplot(x = 'engineSize', y = 'price', data = train, color = 'orange',line_kws={\"color\": \"red\"})\nplt.xlim(-1,7)\n#Note that some of the engineSize values are 0. They are incorrect, and should ideally be imputed before developing the model.\n\n(-1.0, 7.0)\n\n\n\n\n\nPredict the car price for the cars in the test dataset. Datasets to be used: Car_features_test.csv, Car_prices_test.csv\n\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\n\n\n#Using the predict() function associated with the 'model' object to make predictions of car price on test (unknown) data\npred_price = model.predict(testf)#Note that the predict() function finds the predictor 'engineSize' in the testf dataframe, and plugs its values in the regression equation for prediction.\n\nMake a visualization that compares the predicted car prices with the actual car prices\n\nsns.scatterplot(x = testp.price, y = pred_price)\n#In case of a perfect prediction, all the points must lie on the line x = y.\nsns.lineplot(x = [0,testp.price.max()], y = [0,testp.price.max()],color='orange') #Plotting the line x = y.\nplt.xlabel('Actual price')\nplt.ylabel('Predicted price')\n\nText(0, 0.5, 'Predicted price')\n\n\n\n\n\nThe prediction doesn’t look too good. This is because we are just using one predictor - engine size. We can probably improve the model by adding more predictors when we learn multiple linear regression.\nWhat is the RMSE of the predicted car price?\n\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n12995.1064515487\n\n\nThe root mean squared error in predicting car price is around $13k.\nWhat is the residual standard error based on the training data?\n\nnp.sqrt(model.mse_resid)\n\n12810.109175214136\n\n\nThe residual standard error on the training data is close to the RMSE on the test data. This shows that the performance of the model on unknown data is comparable to its performance on known data. This implies that the model is not overfitting, which is good! In case we overfit a model on the training data, its performance on unknown data is likely to be worse than that on the training data.\nFind the confidence and prediction intervals of the predicted car price\n\n#Using the get_prediction() function associated with the 'model' object to get the intervals\nintervals = model.get_prediction(testf)\n\n\n#The function requires specifying alpha (probability of Type 1 error) instead of the confidence level to get the intervals\nintervals.summary_frame(alpha=0.05)\n\n\n\n\n\n  \n    \n      \n      mean\n      mean_se\n      mean_ci_lower\n      mean_ci_upper\n      obs_ci_lower\n      obs_ci_upper\n    \n  \n  \n    \n      0\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      1\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      2\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      3\n      8866.245277\n      316.580850\n      8245.606701\n      9486.883853\n      -16254.905974\n      33987.396528\n    \n    \n      4\n      47831.088340\n      468.949360\n      46911.740050\n      48750.436631\n      22700.782946\n      72961.393735\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2667\n      47831.088340\n      468.949360\n      46911.740050\n      48750.436631\n      22700.782946\n      72961.393735\n    \n    \n      2668\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      2669\n      8866.245277\n      316.580850\n      8245.606701\n      9486.883853\n      -16254.905974\n      33987.396528\n    \n    \n      2670\n      21854.526298\n      184.135754\n      21493.538727\n      22215.513869\n      -3261.551421\n      46970.604017\n    \n    \n      2671\n      21854.526298\n      184.135754\n      21493.538727\n      22215.513869\n      -3261.551421\n      46970.604017\n    \n  \n\n2672 rows × 6 columns\n\n\n\nShow the regression line predicting car price based on engine size for test data. Also show the confidence and prediction intervals for the car price.\n\ninterval_table = intervals.summary_frame(alpha=0.05)\n\n\nsns.scatterplot(x = testf.engineSize, y = pred_price,color = 'orange', s = 10)\nsns.lineplot(x = testf.engineSize, y = pred_price, color = 'red')\nsns.lineplot(x = testf.engineSize, y = interval_table.mean_ci_lower, color = 'blue')\nsns.lineplot(x = testf.engineSize, y = interval_table.mean_ci_upper, color = 'blue',label='_nolegend_')\nsns.lineplot(x = testf.engineSize, y = interval_table.obs_ci_lower, color = 'green')\nsns.lineplot(x = testf.engineSize, y = interval_table.obs_ci_upper, color = 'green')\nplt.legend(labels=[\"Regression line\",\"Confidence interval\", \"Prediction interval\"])\n\n<matplotlib.legend.Legend at 0x26a3a32c550>"
  },
  {
    "objectID": "Lec2_MultipleLinearRegression.html",
    "href": "Lec2_MultipleLinearRegression.html",
    "title": "2  Multiple Linear Regression",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nDevelop a multiple linear regression model that predicts car price based on engine size, year, mileage, and mpg. Datasets to be used: Car_features_train.csv, Car_prices_train.csv\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990\n    \n  \n\n\n\n\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nols_object = smf.ols(formula = 'price~year+mileage+mpg+engineSize', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.660 \n\n\n  Model:                   OLS         Adj. R-squared:        0.660 \n\n\n  Method:             Least Squares    F-statistic:           2410. \n\n\n  Date:             Tue, 27 Dec 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:07:25       Log-Likelihood:      -52497. \n\n\n  No. Observations:        4960        AIC:                1.050e+05\n\n\n  Df Residuals:            4955        BIC:                1.050e+05\n\n\n  Df Model:                   4                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -3.661e+06  1.49e+05   -24.593  0.000 -3.95e+06 -3.37e+06\n\n\n  year        1817.7366    73.751    24.647  0.000  1673.151  1962.322\n\n\n  mileage       -0.1474     0.009   -16.817  0.000    -0.165    -0.130\n\n\n  mpg          -79.3126     9.338    -8.493  0.000   -97.620   -61.006\n\n\n  engineSize  1.218e+04   189.969    64.107  0.000  1.18e+04  1.26e+04\n\n\n\n\n  Omnibus:       2450.973   Durbin-Watson:         0.541 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   31060.548\n\n\n  Skew:            2.045    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.557    Cond. No.           3.83e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nThe model equation is: estimated car price = -3.661e6 + 1818 * year -0.15 * mileage - 79.31 * mpg + 12180 * engineSize\nPredict the car price for the cars in the test dataset. Datasets to be used: Car_features_test.csv, Car_prices_test.csv\n\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\n\n\n#Using the predict() function associated with the 'model' object to make predictions of car price on test (unknown) data\npred_price = model.predict(testf)#Note that the predict() function finds the predictor 'engineSize' in the testf dataframe, and plugs its values in the regression equation for prediction.\n\nMake a visualization that compares the predicted car prices with the actual car prices\n\nsns.scatterplot(x = testp.price, y = pred_price)\n#In case of a perfect prediction, all the points must lie on the line x = y.\nsns.lineplot(x = [0,testp.price.max()], y = [0,testp.price.max()],color='orange') #Plotting the line x = y.\nplt.xlabel('Actual price')\nplt.ylabel('Predicted price')\n\nText(0, 0.5, 'Predicted price')\n\n\n\n\n\nThe prediction looks better as compared to the one with simple linear regression. This is because we have four predictors to help explain the variation in car price, instead of just one in the case of simple linear regression. Also, all the predictors have a significant relationship with price as evident from their p-values. Thus, all four of them are contributing in explaining the variation. Note the higher values of R2 as compared to the one in the case of simple linear regression.\nWhat is the RMSE of the predicted car price?\n\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n9956.82497993548\n\n\nWhat is the residual standard error based on the training data?\n\nnp.sqrt(model.mse_resid)\n\n9563.74782917604\n\n\n\nsns.scatterplot(x = model.fittedvalues, y=model.resid,color = 'orange')\nsns.lineplot(x = [pred_price.min(),pred_price.max()],y = [0,0],color = 'blue')\nplt.xlabel('Predicted price')\nplt.ylabel('Residual')\n\nText(0, 0.5, 'Residual')\n\n\n\n\n\nWill the explained variation (R-squared) in car price always increase if we add a variable?\nShould we keep on adding variables as long as the explained variation (R-squared) is increasing?\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nnp.random.seed(1)\ntrain['rand_col'] = np.random.rand(train.shape[0])\nols_object = smf.ols(formula = 'price~year+mileage+mpg+engineSize+rand_col', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.661 \n\n\n  Model:                   OLS         Adj. R-squared:        0.660 \n\n\n  Method:             Least Squares    F-statistic:           1928. \n\n\n  Date:             Tue, 27 Dec 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:07:38       Log-Likelihood:      -52497. \n\n\n  No. Observations:        4960        AIC:                1.050e+05\n\n\n  Df Residuals:            4954        BIC:                1.050e+05\n\n\n  Df Model:                   5                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -3.662e+06  1.49e+05   -24.600  0.000 -3.95e+06 -3.37e+06\n\n\n  year        1818.1672    73.753    24.652  0.000  1673.578  1962.756\n\n\n  mileage       -0.1474     0.009   -16.809  0.000    -0.165    -0.130\n\n\n  mpg          -79.2837     9.338    -8.490  0.000   -97.591   -60.976\n\n\n  engineSize  1.218e+04   189.972    64.109  0.000  1.18e+04  1.26e+04\n\n\n  rand_col     451.1226   471.897     0.956  0.339  -474.004  1376.249\n\n\n\n\n  Omnibus:       2451.728   Durbin-Watson:         0.541 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   31040.331\n\n\n  Skew:            2.046    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.552    Cond. No.           3.83e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nAdding a variable with random values to the model (rand_col) increased the explained variation (R-squared). This is because the model has one more parameter to tune to reduce the residual squared error (RSS). However, the p-value of rand_col suggests that its coefficient is zero. Thus, using the model with rand_col may give poorer performance on unknown data, as compared to the model without rand_col. This implies that it is not a good idea to blindly add variables in the model to increase R-squared."
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html#variable-interactions",
    "href": "Lec3_VariableTransformations_and_Interactions.html#variable-interactions",
    "title": "3  Variable interactions and transformations",
    "section": "3.1 Variable interactions",
    "text": "3.1 Variable interactions\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990\n    \n  \n\n\n\n\nUntil now, we have assumed that the association between a predictor \\(X_j\\) and response \\(Y\\) does not depend on the value of other predictors. For example, the multiple linear regression model that we developed in Chapter 2 assumes that the average increase in price associated with a unit increase in engineSize is always $12,180, regardless of the value of other predictors. However, this assumption may be incorrect.\n\n3.1.1 Variable interaction between continuous predictors\nWe can relax this assumption by considering another predictor, called an interaction term. Let us assume that the average increase in price associated with a one-unit increase in engineSize depends on the model year of the car. In other words, there is an interaction between engineSize and year. This interaction can be included as a predictor, which is the product of engineSize and year. Note that there are several possible interactions that we can consider. Here the interaction between engineSize and year is just an example.\n\n#Considering interaction between engineSize and year\nols_object = smf.ols(formula = 'price~year*engineSize+mileage+mpg', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.682 \n\n\n  Model:                   OLS         Adj. R-squared:        0.681 \n\n\n  Method:             Least Squares    F-statistic:           2121. \n\n\n  Date:             Tue, 24 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 15:28:11       Log-Likelihood:      -52338. \n\n\n  No. Observations:        4960        AIC:                1.047e+05\n\n\n  Df Residuals:            4954        BIC:                1.047e+05\n\n\n  Df Model:                   5                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                     coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept        5.606e+05  2.74e+05     2.048  0.041   2.4e+04   1.1e+06\n\n\n  year             -275.3833   135.695    -2.029  0.042  -541.405    -9.361\n\n\n  engineSize      -1.796e+06  9.97e+04   -18.019  0.000 -1.99e+06  -1.6e+06\n\n\n  year:engineSize   896.7687    49.431    18.142  0.000   799.861   993.676\n\n\n  mileage            -0.1525     0.008   -17.954  0.000    -0.169    -0.136\n\n\n  mpg               -84.3417     9.048    -9.322  0.000  -102.079   -66.604\n\n\n\n\n  Omnibus:       2330.413   Durbin-Watson:         0.524 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   29977.437\n\n\n  Skew:            1.908    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.423    Cond. No.           7.66e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 7.66e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that the R-squared has increased as compared to the model in Chapter 2 since we added a predictor.\nThe model equation is:\n\\[\\begin{equation}\nprice = \\beta_0 + \\beta_1*year + \\beta_2*engineSize + \\beta_3*(year * engineSize) + \\beta4*mileage + \\beta_5*mpg,\n\\end{equation}\\]or\n\\[\\begin{equation}\nprice = \\beta_0 + \\beta_1*year + (\\beta_2+\\beta_3*year)*engineSize + \\beta4*mileage + \\beta_5*mpg,\n\\end{equation}\\]or\n\\[\\begin{equation}\nprice = \\beta_0 + \\beta_1*year + \\tilde \\beta*engineSize + \\beta4*mileage + \\beta_5*mpg,\n\\end{equation}\\]\nSince \\(\\tilde \\beta\\) is a function of year, the association between engineSize and price is no longer a constant. A change in the value of year will change the association between price and engineSize.\nSubstituting the values of the coefficients: \\[\\begin{equation}\nprice = 5.606e5 - 275.3833*year + (-1.796e6+896.7687*year)*engineSize -0.1525*mileage -84.3417*mpg\n\\end{equation}\\]\nThus, for cars launched in the year 2010, the average increase in price for one liter increase in engine size is -1.796e6 + 896.7687 * 2010 \\(\\approx\\) \\$6,500, assuming all the other predictors are constant. However, for cars launched in the year 2020, the average increase in price for one liter increase in engine size is -1.796e6 + 896.7687*2020 \\(\\approx\\) \\$15,500 , assuming all the other predictors are constant.\nSimilarly, the equation can be re-arranged as: \\[\\begin{equation}\nprice = 5.606e5 +(-275.3833+896.7687*engineSize)*year -1.796e6*engineSize -0.1525*mileage -84.3417*mpg\n\\end{equation}\\]\nThus, for cars with an engine size of 2 litres, the average increase in price for a one year newer model is -275.3833+896.7687 * 2 \\(\\approx\\) \\$1500, assuming all the other predictors are constant. However, for cars with an engine size of 3 litres, the average increase in price for a one year newer model is -275.3833+896.7687 * 3 \\(\\approx\\) \\$2400, assuming all the other predictors are constant.\n\n#Computing the RMSE of the model with the interaction term\npred_price = model.predict(testf)\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n9423.598872501092\n\n\nNote that the RMSE is lower than that of the model in Chapter 2. This is because the interaction term between engineSize and year is significant and relaxes the assumption of constant association between price and engine size, and between price and year. This added flexibility makes the model better fit the data. Caution: Too much flexibility may lead to overfitting!\nNote that interaction terms corresponding to other variable pairs, and higher order interaction terms (such as those containing 3 or 4 variables) may also be significant and improve the model fit & thereby the prediction accuracy of the model.\n\n\n3.1.2 Including qualitative predictors in the model\nLet us develop a model for predicting price based on engineSize and the qualitative predictor transmission.\n\n#checking the distribution of values of transmission\ntrain.transmission.value_counts()\n\nManual       1948\nAutomatic    1660\nSemi-Auto    1351\nOther           1\nName: transmission, dtype: int64\n\n\nNote that the Other category of the variable transmission contains only a single observation, which is likely to be insufficient to train the model. We’ll remove that observation from the training data. Another option may be to combine the observation in the Other category with the nearest category, and keep it in the data.\n\ntrain_updated = train[train.transmission!='Other']\n\n\nols_object = smf.ols(formula = 'price~engineSize+transmission', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.459 \n\n\n  Model:                   OLS         Adj. R-squared:        0.458 \n\n\n  Method:             Least Squares    F-statistic:           1400. \n\n\n  Date:             Tue, 24 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 15:28:21       Log-Likelihood:      -53644. \n\n\n  No. Observations:        4959        AIC:                1.073e+05\n\n\n  Df Residuals:            4955        BIC:                1.073e+05\n\n\n  Df Model:                   3                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                  3042.6765   661.190     4.602  0.000  1746.451  4338.902\n\n\n  transmission[T.Manual]    -6770.6165   442.116   -15.314  0.000 -7637.360 -5903.873\n\n\n  transmission[T.Semi-Auto]  4994.3112   442.989    11.274  0.000  4125.857  5862.765\n\n\n  engineSize                 1.023e+04   247.485    41.323  0.000  9741.581  1.07e+04\n\n\n\n\n  Omnibus:       1575.518   Durbin-Watson:         0.579 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   11006.609\n\n\n  Skew:            1.334    Prob(JB):               0.00 \n\n\n  Kurtosis:        9.793    Cond. No.               11.4 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nNote that there is no coefficient for the Automatic level of the variable Transmission. If a car doesn’t have Manual or Semi-Automatic transmission, then it has an Automatic transmission. Thus, the coefficient of Automatic will be redundant, and the dummy variable corresponding to Automatic transmission is dropped from the model.\nThe level of the categorical variable that is dropped from the model is called the baseline level. Here Automatic transmission is the baseline level. The coefficients of other levels of transmission should be interpreted with respect to the baseline level.\nQ: Interpret the intercept term\nAns: For the hypothetical scenario of a car with zero engine size and Automatic transmission, the estimated mean car price is \\(\\approx\\) \\$3042.\nQ: Interpret the coefficient of transmission[T.Manual]\nAns: The estimated mean price of a car with manual transmission is \\(\\approx\\) \\$6770 less than that of a car with Automatic transmission.\nLet us visualize the developed model.\n\n#Visualizing the developed model\nplt.rcParams[\"figure.figsize\"] = (9,6)\nsns.set(font_scale = 1.3)\nx = np.linspace(train_updated.engineSize.min(),train_updated.engineSize.max(),100)\nax = sns.lineplot(x = x, y = model.params['engineSize']*x+model.params['Intercept'], color = 'red')\nsns.lineplot(x = x, y = model.params['engineSize']*x+model.params['Intercept']+model.params['transmission[T.Semi-Auto]'], color = 'blue')\nsns.lineplot(x = x, y = model.params['engineSize']*x+model.params['Intercept']+model.params['transmission[T.Manual]'], color = 'green')\nplt.legend(labels=[\"Automatic\",\"Semi-Automatic\", \"Manual\"])\nplt.xlabel('Engine size (in litre)')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nBased on the developed model, for a given engine size, the car with a semi-automatic transmission is estimated to be the most expensive on average, while the car with a manual transmission is estimated to be the least expensive on average.\nChanging the baseline level: By default, the baseline level is chosen as the one that comes first if the levels are arranged in alphabetical order. However, you can change the baseline level by specifying one explicitly.\nInternally, statsmodels uses the patsy package to convert formulas and data to the matrices that are used in model fitting. You may refer to this section in the patsy documentation to specify a particular level of the categorical variable as the baseline.\nFor example, suppose we wish to change the baseline level to Manual transmission. We can specify this in the formula as follows:\n\nols_object = smf.ols(formula = 'price~engineSize+C(transmission, Treatment(\"Manual\"))', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.459 \n\n\n  Model:                   OLS         Adj. R-squared:        0.458 \n\n\n  Method:             Least Squares    F-statistic:           1400. \n\n\n  Date:             Tue, 24 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 15:28:39       Log-Likelihood:      -53644. \n\n\n  No. Observations:        4959        AIC:                1.073e+05\n\n\n  Df Residuals:            4955        BIC:                1.073e+05\n\n\n  Df Model:                   3                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                                                       coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                                         -3727.9400   492.917    -7.563  0.000 -4694.275 -2761.605\n\n\n  C(transmission, Treatment(\"Manual\"))[T.Automatic]  6770.6165   442.116    15.314  0.000  5903.873  7637.360\n\n\n  C(transmission, Treatment(\"Manual\"))[T.Semi-Auto]  1.176e+04   473.110    24.867  0.000  1.08e+04  1.27e+04\n\n\n  engineSize                                         1.023e+04   247.485    41.323  0.000  9741.581  1.07e+04\n\n\n\n\n  Omnibus:       1575.518   Durbin-Watson:         0.579 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   11006.609\n\n\n  Skew:            1.334    Prob(JB):               0.00 \n\n\n  Kurtosis:        9.793    Cond. No.               8.62 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n3.1.3 Including qualitative predictors and their interaction with continuous predictors in the model\nNote that the qualitative predictor leads to fitting 3 parallel lines to the data, as there are 3 categories.\nHowever, note that we have made the constant association assumption. The fact that the lines are parallel means that the average increase in car price for one litre increase in engine size does not depend on the type of transmission. This represents a potentially serious limitation of the model, since in fact a change in engine size may have a very different association on the price of an automatic car versus a semi-automatic or manual car.\nThis limitation can be addressed by adding an interaction variable, which is the product of engineSize and the dummy variables for semi-automatic and manual transmissions.\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nols_object = smf.ols(formula = 'price~engineSize*transmission', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.479 \n\n\n  Model:                   OLS         Adj. R-squared:        0.478 \n\n\n  Method:             Least Squares    F-statistic:           909.9 \n\n\n  Date:             Sun, 22 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 22:55:55       Log-Likelihood:      -53550. \n\n\n  No. Observations:        4959        AIC:                1.071e+05\n\n\n  Df Residuals:            4953        BIC:                1.072e+05\n\n\n  Df Model:                   5                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                                          coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                             3754.7238   895.221     4.194  0.000  1999.695  5509.753\n\n\n  transmission[T.Manual]                1768.5856  1294.071     1.367  0.172  -768.366  4305.538\n\n\n  transmission[T.Semi-Auto]            -5282.7164  1416.472    -3.729  0.000 -8059.628 -2505.805\n\n\n  engineSize                            9928.6082   354.511    28.006  0.000  9233.610  1.06e+04\n\n\n  engineSize:transmission[T.Manual]    -5285.9059   646.175    -8.180  0.000 -6552.695 -4019.117\n\n\n  engineSize:transmission[T.Semi-Auto]  4162.2428   552.597     7.532  0.000  3078.908  5245.578\n\n\n\n\n  Omnibus:       1379.846   Durbin-Watson:         0.622\n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   9799.471\n\n\n  Skew:            1.139    Prob(JB):               0.00\n\n\n  Kurtosis:        9.499    Cond. No.               30.8\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe model equation for the model with interactions is:\nAutomatic transmission: \\(price = 3754.7238 + 9928.6082 * engineSize\\),\nSemi-Automatic transmission: \\(price = 3754.7238 + 9928.6082 * engineSize + (-5282.7164+4162.2428*engineSize)\\),\nManual transmission: \\(price = 3754.7238 + 9928.6082 * engineSize +(1768.5856-5285.9059*engineSize)\\), or\nAutomatic transmission: \\(price = 3754.7238 + 9928.6082 * engineSize\\),\nSemi-Automatic transmission: \\(price = -1527 + 7046 * engineSize\\),\nManual transmission: \\(price = 5523 + 4642 * engineSize\\),\nQ: Interpret the coefficient of manual tranmission, i.e., the coefficient of transmission[T.Manual].\nA: For a given engine size, the estimated mean price of a car with Manual transmission is \\(\\approx\\) \\$1768 more than the estimated mean price of a car with Automatic transmission.\nQ: Interpret the coefficient of the interaction between engine size and manual transmission, i.e., the coefficient of engineSize:transmission[T.Manual].\nA: For a unit (or a litre) increase in engineSize , the increase in estimated mean price of a car with Manual transmission is \\(\\approx\\) \\$5285 less than the increase in estimated mean price of a car with Automatic transmission.\n\n#Visualizing the developed model with interaction terms\nplt.rcParams[\"figure.figsize\"] = (9,6)\nsns.set(font_scale = 1.3)\nx = np.linspace(train_updated.engineSize.min(),train_updated.engineSize.max(),100)\nax = sns.lineplot(x = x, y = model.params['engineSize']*x+model.params['Intercept'], label='Automatic', color = 'red')\nplt.plot(x, (model.params['engineSize']+model.params['engineSize:transmission[T.Semi-Auto]'])*x+model.params['Intercept']+model.params['transmission[T.Semi-Auto]'], '-b', label='Semi-Automatic')\nplt.plot(x, (model.params['engineSize']+model.params['engineSize:transmission[T.Manual]'])*x+model.params['Intercept']+model.params['transmission[T.Manual]'], '-g', label='Manual')\nplt.legend(loc='upper left')\nplt.xlabel('Engine size (in litre)')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nNote the interaction term adds flexibility to the model.\nThe slope of the regression line for semi-automatic cars is the largest. This suggests that increase in engine size is associated with a higher increase in car price for semi-automatic cars, as compared to other cars."
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html#variable-transformations",
    "href": "Lec3_VariableTransformations_and_Interactions.html#variable-transformations",
    "title": "3  Variable interactions and transformations",
    "section": "3.2 Variable transformations",
    "text": "3.2 Variable transformations\nSo far we have considered only a linear relationship between the predictors and the response. However, the relationship may be non-linear.\nConsider the regression plot of price on mileage.\n\nax = sns.regplot(x = train_updated.mileage, y =train_updated.price,color = 'orange', line_kws = {'color':'blue'})\nplt.xlabel('Mileage')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\n\n#R-squared of the model with just mileage\nmodel = smf.ols('price~mileage', data = train_updated).fit()\nmodel.rsquared\n\n0.22928048993376182\n\n\nFrom the first scatterplot, we see that the relationship between price and mileage doesn’t seem to be linear, as the points do not lie on a straight line. Also, we see the regression line (or the curve), which is the best fit line doesn’t seem to fit the points well. However, price on average seems to decrease with mileage, albeit in a non-linear manner.\n\n3.2.1 Quadratic transformation\nSo, we guess that if we model price as a quadratic function of mileage, the model may better fit the points (or the curve may better fit the points). Let us transform the predictor mileage to include \\(mileage^2\\) (i.e., perform a quadratic transformation on the predictor).\n\n#Including mileage squared as a predictor and developing the model\nols_object = smf.ols(formula = 'price~mileage+I(mileage**2)', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.271 \n\n\n  Model:                   OLS         Adj. R-squared:        0.271 \n\n\n  Method:             Least Squares    F-statistic:           920.6 \n\n\n  Date:             Sun, 22 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 23:26:05       Log-Likelihood:      -54382. \n\n\n  No. Observations:        4959        AIC:                1.088e+05\n\n\n  Df Residuals:            4956        BIC:                1.088e+05\n\n\n  Df Model:                   2                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                     coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept         3.44e+04   332.710   103.382  0.000  3.37e+04   3.5e+04\n\n\n  mileage            -0.5662     0.017   -33.940  0.000    -0.599    -0.534\n\n\n  I(mileage ** 2)  2.629e-06  1.56e-07    16.813  0.000  2.32e-06  2.94e-06\n\n\n\n\n  Omnibus:       2362.973   Durbin-Watson:         0.325 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   22427.952\n\n\n  Skew:            2.052    Prob(JB):               0.00 \n\n\n  Kurtosis:       12.576    Cond. No.           4.81e+09 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 4.81e+09. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that in the formula specified within the ols() function, the I() operator isolates or insulates the contents within I(…) from the regular formula operators. Without the I() operator, mileage**2 will be treated as the interaction of mileage with itself, which is mileage. Thus, to add the square of mileage as a separate predictor, we need to use the I() operator.\nLet us visualize the model fit with the quadratic transformation of the predictor - mileage.\n\n#Visualizing the regression line with the model consisting of the quadratic transformation of the predictor - mileage\npred_price = model.predict(train_updated)\nax = sns.scatterplot(x = 'mileage', y = 'price', data = train_updated, color = 'orange')\nsns.lineplot(x = train_updated.mileage, y = pred_price, color = 'blue')\nplt.xlabel('Mileage')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\nThe above model seems to better fit the data (as compared to the model without transformation) at least upto mileage around 125,000. The \\(R^2\\) of the model with the quadratic transformation of mileage is also higher than that of the model without transformation indicating a better fit.\n\n\n3.2.2 Cubic transformation\nLet us see if a cubic transformation of mileage can further improve the model fit.\n\n#Including mileage squared and mileage cube as predictors and developing the model\nols_object = smf.ols(formula = 'price~mileage+I(mileage**2)+I(mileage**3)', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.283 \n\n\n  Model:                   OLS         Adj. R-squared:        0.283 \n\n\n  Method:             Least Squares    F-statistic:           652.3 \n\n\n  Date:             Sun, 22 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 23:33:27       Log-Likelihood:      -54340. \n\n\n  No. Observations:        4959        AIC:                1.087e+05\n\n\n  Df Residuals:            4955        BIC:                1.087e+05\n\n\n  Df Model:                   3                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                     coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept        3.598e+04   371.926    96.727  0.000  3.52e+04  3.67e+04\n\n\n  mileage            -0.7742     0.028   -27.634  0.000    -0.829    -0.719\n\n\n  I(mileage ** 2)  6.875e-06  4.87e-07    14.119  0.000  5.92e-06  7.83e-06\n\n\n  I(mileage ** 3) -1.823e-11  1.98e-12    -9.199  0.000 -2.21e-11 -1.43e-11\n\n\n\n\n  Omnibus:       2380.788   Durbin-Watson:         0.321 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   23039.307\n\n\n  Skew:            2.065    Prob(JB):               0.00 \n\n\n  Kurtosis:       12.719    Cond. No.           7.73e+14 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 7.73e+14. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#Visualizing the model with the cubic transformation of mileage\npred_price = model.predict(train_updated)\nax = sns.scatterplot(x = 'mileage', y = 'price', data = train_updated, color = 'orange')\nsns.lineplot(x = train_updated.mileage, y = pred_price, color = 'blue')\nplt.xlabel('Mileage')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\nNote that the model fit with the cubic transformation of mileage seems slightly better as compared to the models with the quadratic transformation, and no transformation of mileage, for mileage up to 180k. However, the model should not be used to predict car prices of cars with a mileage higher than 180k.\nLet’s update the model created earlier (in the beginning of this chapter) to include the transformed predictor.\n\n#Model with an interaction term and a variable transformation term\nols_object = smf.ols(formula = 'price~year*engineSize+mileage+mpg+I(mileage**2)', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.702 \n\n\n  Model:                   OLS         Adj. R-squared:        0.702 \n\n\n  Method:             Least Squares    F-statistic:           1947. \n\n\n  Date:             Sun, 22 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 23:42:13       Log-Likelihood:      -52162. \n\n\n  No. Observations:        4959        AIC:                1.043e+05\n\n\n  Df Residuals:            4952        BIC:                1.044e+05\n\n\n  Df Model:                   6                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                     coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept         1.53e+06   2.7e+05     5.671  0.000     1e+06  2.06e+06\n\n\n  year             -755.7419   133.791    -5.649  0.000 -1018.031  -493.453\n\n\n  engineSize      -2.022e+06  9.72e+04   -20.803  0.000 -2.21e+06 -1.83e+06\n\n\n  year:engineSize  1008.6993    48.196    20.929  0.000   914.215  1103.184\n\n\n  mileage            -0.3548     0.014   -25.973  0.000    -0.382    -0.328\n\n\n  mpg               -54.7450     8.896    -6.154  0.000   -72.185   -37.305\n\n\n  I(mileage ** 2)  1.926e-06  1.04e-07    18.536  0.000  1.72e-06  2.13e-06\n\n\n\n\n  Omnibus:       2355.448   Durbin-Watson:         0.562 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   38317.404\n\n\n  Skew:            1.857    Prob(JB):               0.00 \n\n\n  Kurtosis:       16.101    Cond. No.           6.40e+12 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 6.4e+12. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that the R-squared has increased as compared to the model with just the interaction term.\n\n#Computing RMSE on test data\npred_price = model.predict(testf)\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n9074.494088619422\n\n\nNote that the prediction accuracy of the model has further increased, as the RMSE has reduced. The transformed predictor is statistically significant and provides additional flexibility to better capture the trend in the data, leading to an increase in prediction accuracy."
  },
  {
    "objectID": "Lec4_ModelAssumptions.html#non-linearity-of-data",
    "href": "Lec4_ModelAssumptions.html#non-linearity-of-data",
    "title": "4  Model assumptions",
    "section": "4.1 Non-linearity of data",
    "text": "4.1 Non-linearity of data\nWe have assumed that there is a linear relationship between the predictors and the response. Residual plots, which are scatter plots of residuals vs fitted values, can be used to identify non-linearity. Fitted values are the values estimated by the model on training data, denoted by \\(\\hat y_i\\), and residuals are given by \\(e_i = y_i - \\hat y_i\\).\n\n#Plotting residuals vs fitted values\nplt.rcParams[\"figure.figsize\"] = (9,6)\nsns.set(font_scale=1.25)\nax = sns.scatterplot(x = model.fittedvalues, y=model.resid,color = 'orange')\nsns.lineplot(x = [pred_price.min(),pred_price.max()],y = [0,0],color = 'blue')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nThe model seems to satisfy this assumption, as we do not observe a strong pattern in the residuals around the line Residuals = 0. Residuals are distributed more or less in a similar manner on both sides of the blue line for all fitted values.\nFor the model to satisfy the linearity assumption perfectly, the points above the line (Residuals = 0), should be mirror age of the points below the line, i.e., the blue line in the above plot should act as a mirror.\nWhat to do if there is non-linear association (page 94 of book): If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as \\(log X, \\sqrt X\\), and \\(X^2\\), in the regression model."
  },
  {
    "objectID": "Lec4_ModelAssumptions.html#non-constant-variance-of-error-terms",
    "href": "Lec4_ModelAssumptions.html#non-constant-variance-of-error-terms",
    "title": "4  Model assumptions",
    "section": "4.2 Non-constant variance of error terms",
    "text": "4.2 Non-constant variance of error terms\nThe variance of the error terms is assumed to be constant, i.e., \\(Var(\\epsilon_i) = \\sigma^2\\), and this assumption is used while deriving the standard errors of the regression coefficients. The standard errors in turn are used to test the significant of the predictors, and obtain their confidence interval. Thus, violation of this assumption may lead to incorrect inference. Non-constant variance of error terms, or violation of the constant variance assumption, is called heteroscedasticity.\nThis assumption can be checked by plotting the residuals against fitted values.\n\n#Plotting residuals vs fitted values\nax = sns.scatterplot(x = model.fittedvalues, y=model.resid,color = 'orange')\nsns.lineplot(x = [pred_price.min(),pred_price.max()],y = [0,0],color = 'blue')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nWe see that the variance of errors seems to increase with increase in the fitted values. In such a case a log transformation of the response can resolve the issue to some extent. This is because a log transform will result in a higher shrinkage of larger values.\n\n#Model with an interaction term and a variable transformation term\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:      np.log(price)    R-squared:             0.803\n\n\n  Model:                   OLS         Adj. R-squared:        0.803\n\n\n  Method:             Least Squares    F-statistic:           1834.\n\n\n  Date:             Wed, 25 Jan 2023   Prob (F-statistic):    0.00 \n\n\n  Time:                 11:37:55       Log-Likelihood:      -1173.8\n\n\n  No. Observations:        4960        AIC:                   2372.\n\n\n  Df Residuals:            4948        BIC:                   2450.\n\n\n  Df Model:                  11                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n                        coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept           -238.2125    25.790    -9.237  0.000  -288.773  -187.652\n\n\n  year                   0.1227     0.013     9.608  0.000     0.098     0.148\n\n\n  engineSize            13.8349     5.795     2.387  0.017     2.475    25.195\n\n\n  mileage                0.0005     0.000     3.837  0.000     0.000     0.001\n\n\n  mpg                   -1.2446     0.345    -3.610  0.000    -1.921    -0.569\n\n\n  year:engineSize       -0.0067     0.003    -2.324  0.020    -0.012    -0.001\n\n\n  year:mileage        -2.67e-07   6.8e-08    -3.923  0.000    -4e-07 -1.34e-07\n\n\n  year:mpg               0.0006     0.000     3.591  0.000     0.000     0.001\n\n\n  engineSize:mileage -2.668e-07  4.08e-07    -0.654  0.513 -1.07e-06  5.33e-07\n\n\n  engineSize:mpg         0.0028     0.000     6.842  0.000     0.002     0.004\n\n\n  mileage:mpg         7.235e-08  1.79e-08     4.036  0.000  3.72e-08  1.08e-07\n\n\n  I(mileage ** 2)     1.828e-11  5.64e-12     3.240  0.001  7.22e-12  2.93e-11\n\n\n\n\n  Omnibus:       711.515   Durbin-Watson:         0.498\n\n\n  Prob(Omnibus):  0.000    Jarque-Bera (JB):   2545.807\n\n\n  Skew:           0.699    Prob(JB):               0.00\n\n\n  Kurtosis:       6.220    Cond. No.           1.73e+13\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.73e+13. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that the coefficient of year turns out to be significant (at 5% significance level), unlike in the previous model. Intuitively, the coefficient of year should have been significant, as year has the highest linear correlation of 50% with car price.\nAlthough the R-squared has increased as compared to the previous model, violation of this assumption does not cause bias in the regression coefficients. Thus, there may not be a large improvement in the model fit, unless we add predictor(s) to address heteroscedasticity.\nLet us check the constant variance assumption again.\n\n#Plotting residuals vs fitted values\nsns.scatterplot(x = (model_log.fittedvalues), y=(model_log.resid),color = 'orange')\nsns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [0,0],color = 'blue')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\n\nText(0, 0.5, 'Residuals')\n\n\n\n\n\nNow we observe that the constant variance assumption is satisfied. Let us see the RMSE of this model on test data.\n\n#Computing RMSE on test data\npred_price_log = model_log.predict(testf)\nnp.sqrt(((testp.price - np.exp(pred_price_log))**2).mean())\n\n9094.209503063496\n\n\nNote that the RMSE of the log-transformed model has increased as compared to the model without transformation. Does it mean the log-transformed model is less accurate?\n\n#Computing MAE on test data\npred_price_log = model_log.predict(testf)\n((np.abs(testp.price - np.exp(pred_price_log))).mean())\n\n5268.398904745121\n\n\nAlthough the RMSE has increased a bit for the log-transformed model, the MAE has reduced. This means the log-transformed model does a bit worse on reducing relatively large errors, but does better in reducing the absolute errors on an average.\n\n#Comparing errors of the log-transformed model with the previous model\nerr = np.abs(testp.price - pred_price)\nerr_log = np.abs(testp.price - np.exp(pred_price_log))\nsns.scatterplot(x = err,y = err_log, color = 'orange')\nsns.lineplot(x = [0,100000], y = [0,100000], color = 'blue')\nnp.sum(err_log<err)/len(err)\n\n0.5572604790419161\n\n\n\n\n\nFor 56% of the cars, the log transformed makes a more accurate prediction than the previous model, which is another criterion based on which the log-transformed model is more accurate. However, the conclusion based on RMSE is different. This is because RMSE can be influenced by a few large errors. Thus, RMSE, though sometimes appropriate than other criteria, should not be used as the sole measure to compare the accuracy of models.\n\n#Visualizing the distribution of price and log(price)\nfig = plt.figure()\nfig.subplots_adjust(hspace=0.4, wspace=0.2)\nsns.set(rc = {'figure.figsize':(20,12)})\nsns.set(font_scale = 2)\nax = fig.add_subplot(2, 2, 1)\nsns.histplot(train.price,kde=True)\nax.set(xlabel='price', ylabel='Count')\nax = fig.add_subplot(2, 2, 2)\nsns.histplot(np.log(train.price),kde=True)\nax.set(xlabel='log price', ylabel='Count')\n\n[Text(0.5, 0, 'log price'), Text(0, 0.5, 'Count')]\n\n\n\n\n\nWe can see that the log transformation shrinked the higher values of price, making its distribution closer to normal.\nNote that heterscedasticity can also occur due to model misspecification, i.e., in case of missing predictor(s). Some of the cars are too expensive, which makes the price distribution skewed. Perhaps, the price of expensive cars be better explained by the car model, a predictor that is missing in the current model."
  },
  {
    "objectID": "Lec5_Potential_issues.html#outliers",
    "href": "Lec5_Potential_issues.html#outliers",
    "title": "5  Potential issues",
    "section": "5.1 Outliers",
    "text": "5.1 Outliers\nAn outlier is a point for which the true response (\\(y_i\\)) is far from the value predicted by the model. Residual plots can be used to identify outliers.\nIf the the response at the \\(i^{th}\\) observation is \\(y_i\\), the prediction is \\(\\hat{y}_i\\), then the residual \\(e_i\\) is:\n\\[e_i = y_i - \\hat{y_i}\\]\n\n#Plotting residuals vs fitted values\nsns.set(rc={'figure.figsize':(10,6)})\nsns.scatterplot(x = (model_log.fittedvalues), y=(model_log.resid),color = 'orange')\nsns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [0,0],color = 'blue')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\n\nText(0, 0.5, 'Residuals')\n\n\n\n\n\nSome of the errors may be high. However, it is difficult to decide how large a residual needs to be before we can consider a point to be an outlier. To address this problem, we have standardized residuals, which are defined as:\n\\[r_i = \\frac{e_i}{RSE(\\sqrt{1-h_{ii}})},\\] where \\(r_i\\) is the standardized residual, \\(RSE\\) is the residual standard error, and \\(h_{ii}\\) is the leverage (introduced in the next section) of the \\(i^{th}\\) observation.\nStandardized residuals, allow the residuals to be compared on a standard scale.\nIssue with standardized residuals:, If the observation corresponding to the standardized residual has a high leverage, then it will drag the regression line / plane / hyperplane towards it, thereby influencing the estimate of the residual itself.\nStudentized residuals: To address the issue with standardized residuals, studentized residual for the \\(i^{th}\\) observation is computed as the standardized residual, but with the \\(RSE\\) (residual standard error) computed after removing the \\(i^{th}\\) observation from the data. Studentized residual, \\(t_i\\) for the \\(i^{th}\\) observation is given as:\n\\[t_i = \\frac{e_i}{RSE_{i}(\\sqrt{1-h_{ii}})},\\] where \\(RSE_{i}\\) is the residual standard error of the model developed on the data without the \\(i^{th}\\) observation.\nStudentized residuals follow a \\(t\\) distribution with \\((n–p–2)\\) degrees of freedom. Thus, in general, observations whose studentized residuals have a magnitude higher than 3 are potential outliers.\nLet us find the studentized residuals in our car price prediction model.\n\n#Studentized residuals\nout = model_log.outlier_test()\nout\n\n\n\n\n\n  \n    \n      \n      student_resid\n      unadj_p\n      bonf(p)\n    \n  \n  \n    \n      0\n      -1.164204\n      0.244398\n      1.0\n    \n    \n      1\n      -0.801879\n      0.422661\n      1.0\n    \n    \n      2\n      -1.263820\n      0.206354\n      1.0\n    \n    \n      3\n      -0.614171\n      0.539130\n      1.0\n    \n    \n      4\n      0.027930\n      0.977719\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      4955\n      -0.523361\n      0.600747\n      1.0\n    \n    \n      4956\n      -0.509539\n      0.610397\n      1.0\n    \n    \n      4957\n      -1.718802\n      0.085713\n      1.0\n    \n    \n      4958\n      -0.077595\n      0.938153\n      1.0\n    \n    \n      4959\n      -0.482388\n      0.629551\n      1.0\n    \n  \n\n4960 rows × 3 columns\n\n\n\nStudentized residuals are in the first column of the above table.\n\n#Plotting studentized residuals vs fitted values\nsns.scatterplot(x = (model_log.fittedvalues), y=(out.student_resid),color = 'orange')\nsns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [0,0],color = 'blue')\nplt.xlabel('Fitted values')\nplt.ylabel('Studentized Residuals')\n\nText(0, 0.5, 'Studentized Residuals')\n\n\n\n\n\nPotential outliers: Observations whose studentized residuals have a magnitude greater than 3.\nImpact of outliers: Outliers do not have a large impact on the OLS line / plane / hyperplane. However, outliers do inflate the residual standard error (RSE). RSE in turn is used to compute the standard errors of regression coefficients. As a result, statistically significant variables may appear to be insignificant, and \\(R^2\\) may appear to be lower.\n\n#Number of points with absolute studentized residuals greater than 3\nnp.sum((np.abs(out.student_resid)>3))\n\n86\n\n\nAre there outliers in our example?: In the above plot, there are 86 points with absolute studentized residuals larger than 3. However, most of the predictors are significant and R-squared has a relatively high value of 80%. Thus, even if there are outliers, there is no need to remove them as it is unlikely to change the significance of individual variables. Furthermore, looking into the data, we find that the price of some of the luxury cars such as Mercedez G-class is actually much higher than average. So, the potential outliers in the data do not seem to be due to incorrect data. The high studentized residuals may be due to some deficiency in the model, such as missing predictor(s) (like car model), rather than incorrect data. Thus, we should not remove any data that has an outlying value of log(price).\nSince model seems to be a variable that can explain the price of overly expensive cars, let us include it in the regression model.\n\n#Model with an interaction term and a variable transformation term\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)+model', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:      np.log(price)    R-squared:             0.958\n\n\n  Model:                   OLS         Adj. R-squared:        0.958\n\n\n  Method:             Least Squares    F-statistic:           1122.\n\n\n  Date:             Sat, 28 Jan 2023   Prob (F-statistic):    0.00 \n\n\n  Time:                 22:50:09       Log-Likelihood:       2687.6\n\n\n  No. Observations:        4960        AIC:                  -5173.\n\n\n  Df Residuals:            4859        BIC:                  -4516.\n\n\n  Df Model:                 100                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n                                     coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                        -148.2934    13.310   -11.142  0.000  -174.386  -122.201\n\n\n  model[T. 7 Series]                  0.1876     0.024     7.811  0.000     0.141     0.235\n\n\n  model[T. 8 Series]                  0.4382     0.033    13.150  0.000     0.373     0.504\n\n\n  model[T. A7]                        0.0791     0.023     3.373  0.001     0.033     0.125\n\n\n  model[T. A8]                        0.1393     0.023     6.014  0.000     0.094     0.185\n\n\n  model[T. Agila]                    -1.0863     0.043   -24.993  0.000    -1.172    -1.001\n\n\n  model[T. Amarok]                   -0.0881     0.025    -3.578  0.000    -0.136    -0.040\n\n\n  model[T. Antara]                   -0.7253     0.039   -18.409  0.000    -0.803    -0.648\n\n\n  model[T. Arteon]                   -0.1461     0.022    -6.656  0.000    -0.189    -0.103\n\n\n  model[T. Avensis]                  -0.5870     0.026   -22.359  0.000    -0.639    -0.536\n\n\n  model[T. Beetle]                   -0.5124     0.028   -18.159  0.000    -0.568    -0.457\n\n\n  model[T. CC]                       -0.4038     0.027   -15.093  0.000    -0.456    -0.351\n\n\n  model[T. CLA Class]                -0.0977     0.026    -3.731  0.000    -0.149    -0.046\n\n\n  model[T. CLK]                      -0.3421     0.077    -4.464  0.000    -0.492    -0.192\n\n\n  model[T. CLS Class]                 0.0589     0.021     2.829  0.005     0.018     0.100\n\n\n  model[T. Caddy]                    -0.2763     0.066    -4.156  0.000    -0.407    -0.146\n\n\n  model[T. Caddy Life]               -0.4621     0.057    -8.116  0.000    -0.574    -0.350\n\n\n  model[T. Caddy Maxi Life]          -0.4247     0.030   -14.008  0.000    -0.484    -0.365\n\n\n  model[T. California]                0.6004     0.051    11.736  0.000     0.500     0.701\n\n\n  model[T. Camry]                    -0.2137     0.053    -4.006  0.000    -0.318    -0.109\n\n\n  model[T. Caravelle]                 0.3928     0.026    15.086  0.000     0.342     0.444\n\n\n  model[T. Combo Life]               -0.6212     0.025   -24.673  0.000    -0.671    -0.572\n\n\n  model[T. Edge]                     -0.0858     0.023    -3.797  0.000    -0.130    -0.042\n\n\n  model[T. Eos]                      -0.4208     0.067    -6.287  0.000    -0.552    -0.290\n\n\n  model[T. Fusion]                   -1.0541     0.056   -18.745  0.000    -1.164    -0.944\n\n\n  model[T. G Class]                   0.9873     0.050    19.705  0.000     0.889     1.085\n\n\n  model[T. GL Class]                  0.0374     0.024     1.548  0.122    -0.010     0.085\n\n\n  model[T. GLB Class]                 0.0548     0.051     1.071  0.284    -0.045     0.155\n\n\n  model[T. GLS Class]                 0.3905     0.027    14.701  0.000     0.338     0.443\n\n\n  model[T. GT86]                     -0.2923     0.028   -10.307  0.000    -0.348    -0.237\n\n\n  model[T. GTC]                      -0.7139     0.027   -26.582  0.000    -0.767    -0.661\n\n\n  model[T. Galaxy]                   -0.1930     0.023    -8.574  0.000    -0.237    -0.149\n\n\n  model[T. Getz]                     -0.9877     0.077   -12.791  0.000    -1.139    -0.836\n\n\n  model[T. Grand C-MAX]              -0.5952     0.025   -23.892  0.000    -0.644    -0.546\n\n\n  model[T. Grand Tourneo Connect]    -0.4122     0.032   -12.989  0.000    -0.474    -0.350\n\n\n  model[T. Hilux]                    -0.2168     0.028    -7.614  0.000    -0.273    -0.161\n\n\n  model[T. I40]                      -0.6009     0.027   -22.525  0.000    -0.653    -0.549\n\n\n  model[T. I800]                     -0.5085     0.025   -20.103  0.000    -0.558    -0.459\n\n\n  model[T. IQ]                       -0.6285     0.105    -5.989  0.000    -0.834    -0.423\n\n\n  model[T. IX20]                     -0.8475     0.025   -34.417  0.000    -0.896    -0.799\n\n\n  model[T. IX35]                     -0.5361     0.026   -20.404  0.000    -0.588    -0.485\n\n\n  model[T. Jetta]                    -0.6581     0.036   -18.143  0.000    -0.729    -0.587\n\n\n  model[T. KA]                       -1.1107     0.028   -40.291  0.000    -1.165    -1.057\n\n\n  model[T. Kamiq]                    -0.4256     0.027   -15.814  0.000    -0.478    -0.373\n\n\n  model[T. Land Cruiser]              0.3111     0.031    10.022  0.000     0.250     0.372\n\n\n  model[T. M Class]                   0.1702     0.028     6.056  0.000     0.115     0.225\n\n\n  model[T. M2]                        0.1094     0.039     2.780  0.005     0.032     0.187\n\n\n  model[T. M3]                        0.4258     0.038    11.217  0.000     0.351     0.500\n\n\n  model[T. M4]                        0.2222     0.024     9.331  0.000     0.175     0.269\n\n\n  model[T. M5]                        0.2669     0.042     6.342  0.000     0.184     0.349\n\n\n  model[T. M6]                        0.2260     0.055     4.127  0.000     0.119     0.333\n\n\n  model[T. Mustang]                  -0.2404     0.034    -6.997  0.000    -0.308    -0.173\n\n\n  model[T. PROACE VERSO]             -0.1794     0.047    -3.822  0.000    -0.271    -0.087\n\n\n  model[T. Prius]                    -0.0179     0.025    -0.703  0.482    -0.068     0.032\n\n\n  model[T. Puma]                     -0.3080     0.030   -10.162  0.000    -0.367    -0.249\n\n\n  model[T. Q8]                        0.4356     0.030    14.571  0.000     0.377     0.494\n\n\n  model[T. R8]                        0.7618     0.042    18.316  0.000     0.680     0.843\n\n\n  model[T. RS3]                       0.2657     0.034     7.724  0.000     0.198     0.333\n\n\n  model[T. RS4]                       0.4582     0.037    12.376  0.000     0.386     0.531\n\n\n  model[T. RS5]                       0.3761     0.039     9.586  0.000     0.299     0.453\n\n\n  model[T. RS6]                       0.6186     0.035    17.453  0.000     0.549     0.688\n\n\n  model[T. Rapid]                    -0.8185     0.028   -29.656  0.000    -0.873    -0.764\n\n\n  model[T. Roomster]                 -0.8455     0.059   -14.402  0.000    -0.961    -0.730\n\n\n  model[T. S Class]                   0.3837     0.022    17.722  0.000     0.341     0.426\n\n\n  model[T. S3]                        0.1209     0.051     2.358  0.018     0.020     0.221\n\n\n  model[T. S4]                        0.1040     0.047     2.217  0.027     0.012     0.196\n\n\n  model[T. SLK]                      -0.1320     0.027    -4.974  0.000    -0.184    -0.080\n\n\n  model[T. SQ5]                       0.3406     0.045     7.587  0.000     0.253     0.429\n\n\n  model[T. SQ7]                       0.5074     0.061     8.332  0.000     0.388     0.627\n\n\n  model[T. Santa Fe]                 -0.1063     0.022    -4.898  0.000    -0.149    -0.064\n\n\n  model[T. Scala]                    -0.5650     0.026   -22.036  0.000    -0.615    -0.515\n\n\n  model[T. Scirocco]                 -0.3340     0.022   -15.058  0.000    -0.377    -0.290\n\n\n  model[T. Shuttle]                  -0.1260     0.030    -4.192  0.000    -0.185    -0.067\n\n\n  model[T. Supra]                     0.2607     0.057     4.587  0.000     0.149     0.372\n\n\n  model[T. Tiguan Allspace]          -0.0838     0.026    -3.179  0.001    -0.136    -0.032\n\n\n  model[T. Tourneo Connect]          -0.4612     0.039   -11.764  0.000    -0.538    -0.384\n\n\n  model[T. Tourneo Custom]           -0.2295     0.028    -8.161  0.000    -0.285    -0.174\n\n\n  model[T. V Class]                   0.0901     0.022     4.072  0.000     0.047     0.133\n\n\n  model[T. Verso]                    -0.5033     0.026   -19.178  0.000    -0.555    -0.452\n\n\n  model[T. Vivaro]                   -0.3030     0.043    -7.003  0.000    -0.388    -0.218\n\n\n  model[T. X-CLASS]                  -0.0954     0.027    -3.588  0.000    -0.148    -0.043\n\n\n  model[T. X4]                        0.1121     0.022     5.030  0.000     0.068     0.156\n\n\n  model[T. X6]                        0.3208     0.025    13.021  0.000     0.272     0.369\n\n\n  model[T. X7]                        0.5808     0.031    18.528  0.000     0.519     0.642\n\n\n  model[T. Yeti]                     -0.4874     0.026   -18.577  0.000    -0.539    -0.436\n\n\n  model[T. Z3]                        0.4315     0.072     6.009  0.000     0.291     0.572\n\n\n  model[T. Z4]                       -0.0755     0.026    -2.936  0.003    -0.126    -0.025\n\n\n  model[T. Zafira Tourer]            -0.6458     0.030   -21.452  0.000    -0.705    -0.587\n\n\n  model[T. i3]                        0.0794     0.051     1.552  0.121    -0.021     0.180\n\n\n  model[T. i8]                        0.9079     0.049    18.719  0.000     0.813     1.003\n\n\n  year                                0.0785     0.007    11.899  0.000     0.066     0.091\n\n\n  engineSize                         -0.3401     2.981    -0.114  0.909    -6.184     5.504\n\n\n  mileage                            -0.0004  6.93e-05    -6.212  0.000    -0.001    -0.000\n\n\n  mpg                                -0.5509     0.169    -3.252  0.001    -0.883    -0.219\n\n\n  year:engineSize                     0.0003     0.001     0.180  0.857    -0.003     0.003\n\n\n  year:mileage                     2.097e-07  3.43e-08     6.109  0.000  1.42e-07  2.77e-07\n\n\n  year:mpg                            0.0003  8.39e-05     3.257  0.001     0.000     0.000\n\n\n  engineSize:mileage              -4.066e-07  2.02e-07    -2.017  0.044 -8.02e-07 -1.13e-08\n\n\n  engineSize:mpg                     -0.0015     0.000    -4.025  0.000    -0.002    -0.001\n\n\n  mileage:mpg                       2.56e-08  8.63e-09     2.967  0.003  8.68e-09  4.25e-08\n\n\n  I(mileage ** 2)                  2.332e-11  2.67e-12     8.726  0.000  1.81e-11  2.86e-11\n\n\n\n\n  Omnibus:       773.626   Durbin-Watson:         1.405 \n\n\n  Prob(Omnibus):  0.000    Jarque-Bera (JB):   11028.746\n\n\n  Skew:           0.258    Prob(JB):               0.00 \n\n\n  Kurtosis:      10.287    Cond. No.           1.93e+13 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.93e+13. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#Computing RMSE on test data with car 'model' as one of the predictors\npred_price_log2 = model_log.predict(testf)\nnp.sqrt(((testp.price - np.exp(pred_price_log2))**2).mean())\n\n4252.20045604376\n\n\n\n#Plotting studentized residuals vs fitted values for the model with car 'model' as one of the predictors\nout = model_log.outlier_test()\nsns.scatterplot(x = (model_log.fittedvalues), y=(out.student_resid),color = 'orange')\nsns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [0,0],color = 'blue')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\n\nText(0, 0.5, 'Residuals')\n\n\n\n\n\n\n#Number of points with absolute studentized residuals greater than 3\nnp.sum((np.abs(out.student_resid)>3))\n\n69\n\n\nNote the RMSE has reduced to almost half of its value corresponding to the regression model without the predictor - model. Car model does help better explain the variation in price of cars! The number of points with absolute studentized residuals greater than 3 has also reduced to 69 from 86."
  },
  {
    "objectID": "Lec5_Potential_issues.html#high-leverage-points",
    "href": "Lec5_Potential_issues.html#high-leverage-points",
    "title": "5  Potential issues",
    "section": "5.2 High leverage points",
    "text": "5.2 High leverage points\nHigh leverage points are those with an unsual value of the predictor(s). They have a relatively higher impact on the OLS line / plane / hyperplane, as compared to the outliers.\nLeverage statistic (page 99 of the book): In order to quantify an observation’s leverage, we compute the leverage statistic. A large value of this statistic indicates an observation with high leverage. For simple linear regression, \\[\\begin{equation}\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar x)^2}{\\sum_{i'=1}^{n}(x_{i'} - \\bar x)^2}.\n\\end{equation}\\]\nIt is clear from this equation that \\(h_i\\) increases with the distance of \\(x_i\\) from \\(\\bar x\\).The leverage statistic \\(h_i\\) is always between \\(1/n\\) and \\(1\\), and the average leverage for all the observations is always equal to \\((p+1)/n\\). So if a given observation has a leverage statistic that greatly exceeds \\((p+1)/n\\), then we may suspect that the corresponding point has high leverage.\nInfluential points: Note that if a high leverage point falls in line with the regression line, then it will not effect the regression line. However, it may inflate R-squared and increase the significance of predictors. If a high leverage point falls away from the regression line, then it is also an outlier, and will effect the regression line. The points whose presence significantly effects the regression line are called influential points. A point that is both a high leverage point and an outlier is likely to be an influential point. However, a high leverage point is not necessarily an influential point.\nSource for influential points: https://online.stat.psu.edu/stat501/book/export/html/973\nLet us see if there are any high leverage points in our regression model without the predictor - model.\n\n#Model with an interaction term and a variable transformation term\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:      np.log(price)    R-squared:             0.803\n\n\n  Model:                   OLS         Adj. R-squared:        0.803\n\n\n  Method:             Least Squares    F-statistic:           1834.\n\n\n  Date:             Sun, 29 Jan 2023   Prob (F-statistic):    0.00 \n\n\n  Time:                 01:25:04       Log-Likelihood:      -1173.8\n\n\n  No. Observations:        4960        AIC:                   2372.\n\n\n  Df Residuals:            4948        BIC:                   2450.\n\n\n  Df Model:                  11                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n                        coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept           -238.2125    25.790    -9.237  0.000  -288.773  -187.652\n\n\n  year                   0.1227     0.013     9.608  0.000     0.098     0.148\n\n\n  engineSize            13.8349     5.795     2.387  0.017     2.475    25.195\n\n\n  mileage                0.0005     0.000     3.837  0.000     0.000     0.001\n\n\n  mpg                   -1.2446     0.345    -3.610  0.000    -1.921    -0.569\n\n\n  year:engineSize       -0.0067     0.003    -2.324  0.020    -0.012    -0.001\n\n\n  year:mileage        -2.67e-07   6.8e-08    -3.923  0.000    -4e-07 -1.34e-07\n\n\n  year:mpg               0.0006     0.000     3.591  0.000     0.000     0.001\n\n\n  engineSize:mileage -2.668e-07  4.08e-07    -0.654  0.513 -1.07e-06  5.33e-07\n\n\n  engineSize:mpg         0.0028     0.000     6.842  0.000     0.002     0.004\n\n\n  mileage:mpg         7.235e-08  1.79e-08     4.036  0.000  3.72e-08  1.08e-07\n\n\n  I(mileage ** 2)     1.828e-11  5.64e-12     3.240  0.001  7.22e-12  2.93e-11\n\n\n\n\n  Omnibus:       711.515   Durbin-Watson:         0.498\n\n\n  Prob(Omnibus):  0.000    Jarque-Bera (JB):   2545.807\n\n\n  Skew:           0.699    Prob(JB):               0.00\n\n\n  Kurtosis:       6.220    Cond. No.           1.73e+13\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.73e+13. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#Computing the leverage statistic for each observation\ninfluence = model_log.get_influence()\nleverage = influence.hat_matrix_diag\n\n\n#Visualizng leverage against studentized residuals\nsns.set(rc={'figure.figsize':(15,8)})\nsm.graphics.influence_plot(model_log);\n\n\n\n\nLet us identify the high leverage points in the data, as they may be effecting the regression line if they are outliers as well, i.e., if they are influential points. Note that there is no defined threshold for a point to be classified as a high leverage point. Some statisticians consider points having twice the average leverage as high leverage points, some consider points having thrice the average leverage as high leverage points, and so on.\n\nout = model_log.outlier_test()\n\n\n#Average leverage of points\n(model_log.df_model+1)/model_log.nobs\n\n0.0024193548387096775\n\n\nLet us consider points having four times the average leverage as high leverage points.\n\n#We will remove all observations that have leverage higher than the threshold value.\nhigh_leverage_threshold = 4*(model_log.df_model+1)/model_log.nobs\n\n\n#Number of high leverage points in the dataset\nnp.sum(leverage>high_leverage_threshold)\n\n197\n\n\nObservations that are both high leverage points and outliers are influential points that may effect the regression line. Let’s remove these influential points from the data and see if it improves the model prediction accuracy on test data.\n\n#Dropping influential points from data\ntrain_filtered = train.drop(np.intersect1d(np.where(np.abs(out.student_resid)>3)[0],\n                                           (np.where(leverage>high_leverage_threshold)[0])))\n\n\ntrain_filtered.shape\n\n(4921, 11)\n\n\n\n#Number of points removed as they were influential\ntrain.shape[0]-train_filtered.shape[0]\n\n39\n\n\nWe removed 39 influential data points from the training data.\n\n#Model the model after removing the high leverage observations\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train_filtered)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:      np.log(price)    R-squared:             0.830\n\n\n  Model:                   OLS         Adj. R-squared:        0.829\n\n\n  Method:             Least Squares    F-statistic:           2173.\n\n\n  Date:             Sun, 29 Jan 2023   Prob (F-statistic):    0.00 \n\n\n  Time:                 01:26:25       Log-Likelihood:      -775.51\n\n\n  No. Observations:        4921        AIC:                   1575.\n\n\n  Df Residuals:            4909        BIC:                   1653.\n\n\n  Df Model:                  11                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n                        coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept           -262.7743    24.455   -10.745  0.000  -310.717  -214.832\n\n\n  year                   0.1350     0.012    11.148  0.000     0.111     0.159\n\n\n  engineSize            16.6645     5.482     3.040  0.002     5.917    27.412\n\n\n  mileage                0.0008     0.000     5.945  0.000     0.001     0.001\n\n\n  mpg                   -1.1217     0.324    -3.458  0.001    -1.758    -0.486\n\n\n  year:engineSize       -0.0081     0.003    -2.997  0.003    -0.013    -0.003\n\n\n  year:mileage       -3.927e-07   6.5e-08    -6.037  0.000  -5.2e-07 -2.65e-07\n\n\n  year:mpg               0.0005     0.000     3.411  0.001     0.000     0.001\n\n\n  engineSize:mileage -4.566e-07  3.86e-07    -1.183  0.237 -1.21e-06     3e-07\n\n\n  engineSize:mpg         0.0071     0.000    16.202  0.000     0.006     0.008\n\n\n  mileage:mpg          7.29e-08  1.68e-08     4.349  0.000     4e-08  1.06e-07\n\n\n  I(mileage ** 2)     1.418e-11  5.29e-12     2.683  0.007  3.82e-12  2.46e-11\n\n\n\n\n  Omnibus:       631.414   Durbin-Watson:         0.553\n\n\n  Prob(Omnibus):  0.000    Jarque-Bera (JB):   1851.015\n\n\n  Skew:           0.682    Prob(JB):               0.00\n\n\n  Kurtosis:       5.677    Cond. No.           1.73e+13\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.73e+13. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that we obtain a higher R-sqauared value of 83% as compared to 80% with the complete data. Removing the influential points helped obtain a better model fit. However, that may also happen just by reducing observations.\n\n#Computing RMSE on test data\npred_price_log = model_log.predict(testf)\nnp.sqrt(((testp.price - np.exp(pred_price_log))**2).mean())\n\n8820.685844070766\n\n\nThe RMSE on test data has also reduced. This shows that some of the influential points were impacting the regression line. With those points removed, the model better captures the general trend in the data."
  },
  {
    "objectID": "Lec5_Potential_issues.html#collinearity",
    "href": "Lec5_Potential_issues.html#collinearity",
    "title": "5  Potential issues",
    "section": "5.3 Collinearity",
    "text": "5.3 Collinearity\nCollinearity refers to the situation when two or more predictor variables have a high linear association. Linear association between a pair of variables can be measured by the correlation coefficient. Thus the correlation matrix can indicate some potential collinearity problems.\nWhy and how is collinearity a problem (page 100-101 of book): The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response.\nSince collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for \\(\\hat \\beta_j\\) to grow. Recall that the t-statistic for each predictor is calculated by dividing \\(\\hat \\beta_j\\) by its standard error. Consequently, collinearity results in a decline in the \\(t\\)-statistic. As a result, in the presence of collinearity, we may fail to reject \\(H_0: \\beta_j = 0\\). This means that the power of the hypothesis test—the probability of correctly detecting a non-zero coefficient—is reduced by collinearity.\nHow to measure collinearity/multicollinearity (page 102 of book): Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation multicollinearity. Instead of inspecting the correlation matrix, a better way to assess multi- collinearity is to compute the variance inflation factor (VIF). The VIF is variance inflation factor the ratio of the variance of \\(\\hat \\beta_j\\) when fitting the full model divided by the variance of \\(\\hat \\beta_j\\) if fit on its own. The smallest possible value for VIF is 1, which indicates the complete absence of collinearity. Typically in practice there is a small amount of collinearity among the predictors. As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. The VIF for each variable can be computed using the formula:\n\\[\\begin{equation}\nVIF(\\hat \\beta_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\n\\end{equation}\\]\n\n#Correlation matrix\ntrain.corr()\n\n\n\n\n\n  \n    \n      \n      carID\n      year\n      mileage\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      carID\n      1.000000\n      0.006251\n      -0.001320\n      0.023806\n      -0.010774\n      0.011365\n      0.012129\n    \n    \n      year\n      0.006251\n      1.000000\n      -0.768058\n      -0.205902\n      -0.057093\n      0.014623\n      0.501296\n    \n    \n      mileage\n      -0.001320\n      -0.768058\n      1.000000\n      0.133744\n      0.125376\n      -0.006459\n      -0.478705\n    \n    \n      tax\n      0.023806\n      -0.205902\n      0.133744\n      1.000000\n      -0.488002\n      0.465282\n      0.144652\n    \n    \n      mpg\n      -0.010774\n      -0.057093\n      0.125376\n      -0.488002\n      1.000000\n      -0.419417\n      -0.369919\n    \n    \n      engineSize\n      0.011365\n      0.014623\n      -0.006459\n      0.465282\n      -0.419417\n      1.000000\n      0.624899\n    \n    \n      price\n      0.012129\n      0.501296\n      -0.478705\n      0.144652\n      -0.369919\n      0.624899\n      1.000000\n    \n  \n\n\n\n\nLet us compute the Variance Inflation Factor (VIF) for the four predictors.\n\nX = train[['mpg','year','mileage','engineSize']]\n\n\nX.columns[1:]\n\nIndex(['mpg', 'year', 'mileage', 'engineSize'], dtype='object')\n\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nX = add_constant(X)\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\n      \n# calculating VIF for each feature\n#vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n#                          for i in range(len(X.columns))]\n\nfor i in range(len(X.columns)):\n    vif_data.loc[i,'VIF'] = variance_inflation_factor(X.values, i)\n\nprint(vif_data)\n\n      feature           VIF\n0       const  1.201579e+06\n1         mpg  1.243040e+00\n2        year  2.452891e+00\n3     mileage  2.490210e+00\n4  engineSize  1.219170e+00\n\n\nAs all the values of VIF are close to one, we do not have the problem of multicollinearity in the model. Note that the VIF of year and mileage is relatively high as they are the most correlated.\n\n#Manually computing the VIF for year\nols_object = smf.ols(formula = 'year~(mpg+engineSize+mileage)', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          year         R-squared:             0.592 \n\n\n  Model:                   OLS         Adj. R-squared:        0.592 \n\n\n  Method:             Least Squares    F-statistic:           2400. \n\n\n  Date:             Tue, 01 Feb 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:24:20       Log-Likelihood:      -10066. \n\n\n  No. Observations:        4960        AIC:                2.014e+04\n\n\n  Df Residuals:            4956        BIC:                2.017e+04\n\n\n  Df Model:                   3                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept   2018.3135     0.140  1.44e+04  0.000  2018.039  2018.588\n\n\n  mpg            0.0095     0.002     5.301  0.000     0.006     0.013\n\n\n  engineSize     0.1171     0.037     3.203  0.001     0.045     0.189\n\n\n  mileage    -9.139e-05  1.08e-06   -84.615  0.000 -9.35e-05 -8.93e-05\n\n\n\n\n  Omnibus:       2949.664   Durbin-Watson:         1.161 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   63773.271\n\n\n  Skew:           -2.426    Prob(JB):               0.00 \n\n\n  Kurtosis:       19.883    Cond. No.           1.91e+05 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.91e+05. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#VIF for year\n1/(1-0.592)\n\n2.4509803921568625\n\n\nNote that year and mileage have a high linear correlation. Removing one of them should decrease the standard error of the coefficient of the other, without significanty decrease R-squared.\n\nols_object = smf.ols(formula = '(price)~(mpg+engineSize+mileage+year)', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.660 \n\n\n  Model:                   OLS         Adj. R-squared:        0.660 \n\n\n  Method:             Least Squares    F-statistic:           2410. \n\n\n  Date:             Sun, 23 Jan 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:45:55       Log-Likelihood:      -52497. \n\n\n  No. Observations:        4960        AIC:                1.050e+05\n\n\n  Df Residuals:            4955        BIC:                1.050e+05\n\n\n  Df Model:                   4                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -3.661e+06  1.49e+05   -24.593  0.000 -3.95e+06 -3.37e+06\n\n\n  mpg          -79.3126     9.338    -8.493  0.000   -97.620   -61.006\n\n\n  engineSize  1.218e+04   189.969    64.107  0.000  1.18e+04  1.26e+04\n\n\n  mileage       -0.1474     0.009   -16.817  0.000    -0.165    -0.130\n\n\n  year        1817.7366    73.751    24.647  0.000  1673.151  1962.322\n\n\n\n\n  Omnibus:       2450.973   Durbin-Watson:         0.541 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   31060.548\n\n\n  Skew:            2.045    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.557    Cond. No.           3.83e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nRemoving mileage from the above regression.\n\nols_object = smf.ols(formula = '(price)~(mpg+engineSize+year)', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.641 \n\n\n  Model:                   OLS         Adj. R-squared:        0.641 \n\n\n  Method:             Least Squares    F-statistic:           2951. \n\n\n  Date:             Mon, 24 Jan 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 00:04:28       Log-Likelihood:      -52635. \n\n\n  No. Observations:        4960        AIC:                1.053e+05\n\n\n  Df Residuals:            4956        BIC:                1.053e+05\n\n\n  Df Model:                   3                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -5.586e+06  9.78e+04   -57.098  0.000 -5.78e+06 -5.39e+06\n\n\n  mpg         -101.9120     9.500   -10.727  0.000  -120.536   -83.288\n\n\n  engineSize  1.196e+04   194.848    61.392  0.000  1.16e+04  1.23e+04\n\n\n  year        2771.1844    48.492    57.147  0.000  2676.118  2866.251\n\n\n\n\n  Omnibus:       2389.075   Durbin-Watson:         0.528 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   26920.051\n\n\n  Skew:            2.018    Prob(JB):               0.00 \n\n\n  Kurtosis:       13.675    Cond. No.           1.41e+06 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.41e+06. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\nols_object = smf.ols(formula = '(price)~(mpg+engineSize+year+mileage)', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.660 \n\n\n  Model:                   OLS         Adj. R-squared:        0.660 \n\n\n  Method:             Least Squares    F-statistic:           2410. \n\n\n  Date:             Mon, 31 Jan 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 12:10:59       Log-Likelihood:      -52497. \n\n\n  No. Observations:        4960        AIC:                1.050e+05\n\n\n  Df Residuals:            4955        BIC:                1.050e+05\n\n\n  Df Model:                   4                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -3.661e+06  1.49e+05   -24.593  0.000 -3.95e+06 -3.37e+06\n\n\n  mpg          -79.3126     9.338    -8.493  0.000   -97.620   -61.006\n\n\n  engineSize  1.218e+04   189.969    64.107  0.000  1.18e+04  1.26e+04\n\n\n  year        1817.7366    73.751    24.647  0.000  1673.151  1962.322\n\n\n  mileage       -0.1474     0.009   -16.817  0.000    -0.165    -0.130\n\n\n\n\n  Omnibus:       2450.973   Durbin-Watson:         0.541 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   31060.548\n\n\n  Skew:            2.045    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.557    Cond. No.           3.83e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\nols_object = smf.ols(formula = '(price)~(year)', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.251 \n\n\n  Model:                   OLS         Adj. R-squared:        0.251 \n\n\n  Method:             Least Squares    F-statistic:           1664. \n\n\n  Date:             Mon, 31 Jan 2022   Prob (F-statistic): 5.84e-314\n\n\n  Time:                 12:11:12       Log-Likelihood:      -54459. \n\n\n  No. Observations:        4960        AIC:                1.089e+05\n\n\n  Df Residuals:            4958        BIC:                1.089e+05\n\n\n  Df Model:                   1                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept -5.728e+06  1.41e+05   -40.627  0.000    -6e+06 -5.45e+06\n\n\n  year       2851.7747    69.907    40.794  0.000  2714.725  2988.824\n\n\n\n\n  Omnibus:       2541.815   Durbin-Watson:         0.228 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   23237.973\n\n\n  Skew:            2.270    Prob(JB):               0.00 \n\n\n  Kurtosis:       12.583    Cond. No.           1.41e+06 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.41e+06. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n73.751/69.907\n\n1.0549873403235728\n\n\n\n1.05**2\n\n1.1025\n\n\nNote that the standard error of the coefficient of year has reduced from 73 to 48, without any large reduction in R-squared."
  },
  {
    "objectID": "Assignment 1.html#instructions",
    "href": "Assignment 1.html#instructions",
    "title": "Appendix A — Assignment A",
    "section": "Instructions",
    "text": "Instructions\n\nYou may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Tuesday, 17th January 2023 at 11:59 pm.\nThere is a bonus question worth 5 points.\nFive points are for properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (1 pt); If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g., printouts of the working directory should not be included in the final submission (1 pt).\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt).\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt).\n\n\nThe maximum possible score in the assigment is 95 + 5 (formatting) + 5 (bonus question) = 105 out of 100. There is no partial credit for the bonus question."
  },
  {
    "objectID": "Assignment 1.html#regression-vs-classification-prediction-vs-inference",
    "href": "Assignment 1.html#regression-vs-classification-prediction-vs-inference",
    "title": "Appendix A — Assignment A",
    "section": "A.1 Regression vs Classification; Prediction vs Inference",
    "text": "A.1 Regression vs Classification; Prediction vs Inference\nExplain (1) whether each scenario is a classification or regression problem, and (2) whether we are most interested in inference or prediction. Answers to both parts must be supported by a justification.\n\nA.1.1 \nConsider a company that is interested in conducting a marketing campaign. The goal is to identify individuals who are likely to respond positively to a marketing campaign, based on observations of demographic variables (such as age, gender, income, etc.) measured on each individual.\n(2+2 points)\n\n\nA.1.2 \nConsider that the company mentioned in the previous question is interested in understanding the impact of advertising promotions in different media types on the company sales. For example, the company is interested in the question, ‘how large of an increase in sales is associated with a given increase in radio vis-a-vis TV advertising?’\n(2+2 points)\n\n\nA.1.3 \nConsider a company selling furniture is interested in the finding the association between demographic characterisitcs of customers (such as age, gender, income, etc.) and their probability of purchase of a particular company product.\n(2+2 points)\n\n\nA.1.4 \nWe are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2022. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.\n(2+2 points)"
  },
  {
    "objectID": "Assignment 1.html#rmse-vs-mae",
    "href": "Assignment 1.html#rmse-vs-mae",
    "title": "Appendix A — Assignment A",
    "section": "A.2 RMSE vs MAE",
    "text": "A.2 RMSE vs MAE\n\nA.2.1 \nDescribe a regression problem, where it will be more appropriate to assess the model accuracy using the root mean squared error (RMSE) metric as compared to the mean absolute error (MAE) metric.\nNote: Don’t use the examples presented in class\n(4 points)\n\n\nA.2.2 \nDescribe a regression problem, where it will be more appropriate to assess the model accuracy using the mean absolute error (MAE) metric as compared to the root mean squared error (RMSE) metric.\nNote: Don’t use the examples presented in class\n(4 points)"
  },
  {
    "objectID": "Assignment 1.html#fnr-vs-fpr",
    "href": "Assignment 1.html#fnr-vs-fpr",
    "title": "Appendix A — Assignment A",
    "section": "A.3 FNR vs FPR",
    "text": "A.3 FNR vs FPR\n\nA.3.1 \nA classification model is developed to predict those customers who will respond positively to a company’s tele-marketing campaign. All those customers that are predicted to respond positively to the campaign will be called by phone to buy the product being marketed. If the customer being called purchases the product (\\(y = 1\\)), the company will get a profit of $100. On the other hand, if they are called and they don’t purchase (\\(y = 0\\)), the company will have a loss of $1. Among FPR (False positive rate) and FNR (False negative rate), which metric is more important to be minimized to reduce the loss associated with misclassification? Justify your answer.\nIn your justification, you must clearly interpret False Negatives (FN) and False Postives (FP) first.\nAssumption: Assume that based on the past marketing campaigns, around 50% of the customers will actually respond positively to the campaign.\n(4 points)\n\n\nA.3.2 \nCan the answer to the previous question change if the assumption stated in the question is false? Justify your answer.\n(6 points)"
  },
  {
    "objectID": "Assignment 1.html#petrol-consumption",
    "href": "Assignment 1.html#petrol-consumption",
    "title": "Appendix A — Assignment A",
    "section": "A.4 Petrol consumption",
    "text": "A.4 Petrol consumption\nRead the dataset petrol_consumption_train.csv. It contains the following five columns:\nPetrol_tax: Petrol tax (cents per gallon)\nPer_capita_income: Average income (dollars)\nPaved_highways: Paved Highways (miles)\nProp_license: Proportion of population with driver’s licenses\nPetrol_consumption: Consumption of petrol (millions of gallons)\n\nA.4.1 \nMake a pairwise plot of all the variables in the dataset. Which variable seems to have the highest linear correlation with Petrol_consumption? Let this variable be predictor P. Note: If you cannot figure out P by looking at the visualization, you may find the pairwise linear correlation coefficient to identify P.\n(4 points)\n\n\nA.4.2 \nFit a simple linear regression model to predict Petrol_consumption based on predictor P (identified in the previous part). Print the model summary.\n(4 points)\n\n\nA.4.3 \nInterpret the coefficient of P. What is the increase in petrol consumption for an increase of 0.05 in P?\n(2+2 points)\n\n\nA.4.4 \nDoes petrol consumption have a statistically significant relationship with the predictor P? Justify your answer.\n(4 points)\n\n\nA.4.5 \nWhat is the R-squared? Interpret its value.\n(4 points)\n\n\nA.4.6 \nUse the model developed above to estimate the petrol consumption for a state in which 50% of the population has a driver’s license. What are the confidence and prediction intervals for your estimate? Which interval includes the irreducible error?\n(4+3+3+2 = 12 points)\n\n\nA.4.7 \nUse the model developed above to estimate the petrol consumption for a state in which 10% of the population has a driver’s license. Are you getting a reasonable estimate? Why or why not?\n(5 points)\n\n\nA.4.8 \nWhat is the residual standard error of the model?\n(4 points)\n\n\nA.4.9 \nUsing the model developed above, predict the petrol consumption for the observations in petrol_consumption_test.csv. Find the RMSE (Root mean squared error). Include the units of RMSE in your answer.\n(5 points)\n\n\nA.4.10 \nBased on the answers to the previous two questions, do you think the model is overfitting? Justify your answer.\n(4 points)\nMake a scatterplot of Petrol_consumption vs Prop_license using petrol_consumption_test.csv. Over the scatterplot, plot the regression line, the prediction interval, and the confidence interval. Distinguish the regression line, prediction interval lines, and confidence interval lines with the following colors. Include the legend as well.\n\nRegression line: red\nConfidence interval lines: blue\nPrediction interval lines: green\n\n(4 points)\nAmong the confidence and prediction intervals, which interval is wider, and why?\n(1+2 points)\n\n\nA.4.11 \nFind the correlation between Petrol_consumption and the rest of the variables in petrol_consumption_train.csv. Based on the correlations, a simple linear regression model with which predictor will have the least R-squared value for predicting Petrol_consumption. Don’t develop any linear regression models.\n(4 points)\nBonus point question\n(5 points - no partial credit)\n\n\nA.4.12 \nFit a simple linear regression model to predict Petrol_consumption based on predictor P, but without an intercept term.\n(you must answer this correctly to qualify for earning bonus points)\n\n\nA.4.13 \nEstimate the petrol consumption for the observations in petrol_consumption_test.csv using the model in developed in the previous question. Find the RMSE.\n(you must answer this correctly to qualify for earning bonus points)\n\n\nA.4.14 \nThe RMSE for the models with and without the intercept are similar, which indicates that both models are almost equally good. However, the R-squared for the model without intercept is much higher than the R-squared for the model with the intercept. Why? Justify your answer.\n(5 points)"
  },
  {
    "objectID": "Assignment B.html#instructions",
    "href": "Assignment B.html#instructions",
    "title": "Appendix B — Assignment B",
    "section": "Instructions",
    "text": "Instructions\n\nYou may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Thursday, 26th January 2023 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (1 pt). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission (1 pt)\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)"
  },
  {
    "objectID": "Assignment B.html#multiple-linear-regression",
    "href": "Assignment B.html#multiple-linear-regression",
    "title": "Appendix B — Assignment B",
    "section": "B.1 Multiple linear regression",
    "text": "B.1 Multiple linear regression\nA study was conducted on 97 men with prostate cancer who were due to receive a radical prostatectomy. The dataset prostate.csv contains data on 9 measurements made on these 97 men. The description of variables can be found here:\n\nB.1.1 Training MLR\nFit a linear regression model with lpsa as the response and all the other variables as predictors. Write down the equation to predict lpsa based on the other eight variables.\n(2+2 points)\n\n\nB.1.2 Model significance\nIs the overall regression significant at 5% level? Justify your answer.\n(2 points)\n\n\nB.1.3 Coefficient interpretation\nInterpret the coefficient of svi.\n(2 points)\n\n\nB.1.4 Variable significance\nReport the \\(p\\)-values for gleason and age. What do you conclude about the significance of these variables?\n(2+2 points)\n\n\nB.1.5 Variable significance from confidence interval\nWhat is the 95% confidence interval for the coefficient of age? Can you conclude anything about its significance based on the confidence interval?\n(2+2 points)\n\n\nB.1.6 \\(p\\)-value\nFit a simple linear regression on lpsa against gleason. What is the \\(p\\)-value for gleason?\n(1+1 points)\n\n\nB.1.7 Predictor significance in presence / absence of other predictors\nIs the predictor gleason statistically significant in the model developed in the previous question (B.1.6)?\nWas gleason statistically significant in the model developed in the first question (B.1.1) with multiple predictors?\nDid the statistical significance of gleason change in the absence of other predictors? Why or why not?\n(1+1+4 points)\n\n\nB.1.8 Prediction\nPredict lpsa of a 65-year old man with lcavol = 1.35, lweight = 3.65, lbph = 0.1, svi = 0.22, lcp = -0.18, gleason = 6.75, and pgg45 = 25 and find 95% prediction intervals.\n(2 points)\n\n\nB.1.9 Variable selection\nFind the largest subset of predictors in the model developed in the first question (B.1.1), such that their coefficients are zero, i.e., none of the predictors in the subset are statistically significant.\nDoes the model \\(R\\)-squared change a lot if you remove the set of predictors identifed above from the model in the first question (B.1.1)?\nHint: You may use the f_test() method to test hypotheses.\n(4+1 points)"
  },
  {
    "objectID": "Assignment B.html#using-mlr-coefficients-and-variable-transformation",
    "href": "Assignment B.html#using-mlr-coefficients-and-variable-transformation",
    "title": "Appendix B — Assignment B",
    "section": "B.2 Using MLR coefficients and variable transformation",
    "text": "B.2 Using MLR coefficients and variable transformation\nThe dataset infmort.csv gives the infant mortality of different countries in the world. The column mortality contains the infant mortality in deaths per 1000 births.\n\nB.2.1 Data visualisation\nMake the following plots:\n\na boxplot of log(mortality) against region (note that a plot of log(mortality) against region better distinguishes the mortality among regions as compared to a plot of mortality against region,\na boxplot of income against region, and\na scatter plot of mortality against income.\n\nWhat trends do you see in these plots? Mention the trend separately for each plot.\n(3+2 points)\n\n\nB.2.2 Removing effect of predictor from response\nEurope seems to have the lowest infant mortality, but it also has the highest per capita annual income. We want to see if Europe still has the lowest mortality if we remove the effect of income from the mortality. We will answer this question with the following steps.\n\nB.2.2.1 Variable transformation\nPlot:\n\nmortality against income,\nlog(mortality) against income,\nmortality against log(income), and\nlog(mortality) against log(income).\n\nBased on the plots, postulate an appropriate model to predict mortality as a function of income. Print the model summary.\n(2+4 points)\n\n\nB.2.2.2 Model update\nUpdate the model developed in the previous question by adding region as a predictor. Print the model summary.\n(2 points)\nUse the model developed in the previous question to compute adjusted_mortality for each observation in the data, where adjusted mortality is the mortality after removing the estimated effect of income. Make a boxplot of log(adjusted_mortality) against region.\n(4+2 points)\n\n\n\nB.2.3 Data visualisation after removing effect of predictor from response\nFrom the plot in the previous question:\n\nDoes Europe still seem to have the lowest mortality as compared to other regions after removing the effect of income from mortality?\nAfter adjusting for income, is there any change in the mortality comparison among different regions. Compare the plot developed in the previous question to the plot of log(mortality) against region developed earlier (B.2.1) to answer this question.\n\nHint: Do any African / Asian / American countries seem to do better than all the European countries with regard to mortality after adjusting for income?\n(1+3 points)"
  },
  {
    "objectID": "Assignment B.html#variable-transformations-and-interactions",
    "href": "Assignment B.html#variable-transformations-and-interactions",
    "title": "Appendix B — Assignment B",
    "section": "B.3 Variable transformations and interactions",
    "text": "B.3 Variable transformations and interactions\nThe dataset soc_ind.csv contains the GDP per capita of some countries along with several social indicators.\n\nB.3.1 Training SLR\nFor a simple linear regression model predicting gdpPerCapita. Which predictor will provide the best model fit (ignore categorical predictors)? Let that predictor be \\(P\\).\n(2 points)\n\n\nB.3.2 Linearity in relationship\nMake a scatterplot of gdpPerCapita vs \\(P\\). Does the relationship between gdpPerCapita and \\(P\\) seem linear or non-linear?\n(1 + 2 points)\n\n\nB.3.3 Variable transformation\nIf the relationship identified in the previous question is non-linear, identify and include transformation(s) of the predictor \\(P\\) in the model to improve the model fit.\nMention the predictors of the transformed model, and report the change in the \\(R\\)-squared value of the transformed model as compared to the simple linear regression model with only \\(P\\).\n(4+4 points)\n\n\nB.3.4 Model visualisation with transformed predictor\nPlot the regression curve of the transformed model (developed in the previous question) over the scatterplot in (b) to visualize model fit. Also make the regression line of the simple linear regression model with only \\(P\\) on the same plot.\n(3 + 1 points)\n\n\nB.3.5 Training MLR with qualitative predictor\nDevelop a model to predict gdpPerCapita with \\(P\\) and continent as predictors.\n\nInterpert the intercept term.\nFor a given value of \\(P\\), are there any continents that do not have a signficant difference between their mean gdpPerCapita and that of Africa? If yes, then which ones, and why? If no, then why not? Consider a significance level of 5%.\n\n(4 + 4 points)\n\n\nB.3.6 Variable interaction\nThe model developed in the previous question has a limitation. It assumes that the increase in mean gdpPerCapita with a unit increase in \\(P\\) does not depend on the continent.\n\nEliminate this limitation by including interaction of continent with \\(P\\) in the model developed in the previous question. Print the model summary of the model with interactions.\nInterpret the coefficient of any one of the interaction terms.\n\n(4 + 4 points)\n\n\nB.3.7 Model visualisation with qualitative predictor\nUse the model developed in the previous question to plot the regression lines for Africa, Asia, and Europe. Put gdpPerCapita on the vertical axis and \\(P\\) on the horizontal axis. Use a legend to distinguish among the regression lines of the three continents.\n(4 points)\n\n\nB.3.8 Model interpretation\nBased on the plot develop in the previous question, which continent has the highest increase in mean gdpPerCapita for a unit increase in \\(P\\), and which one has the least? Justify your answer.\n(2+2 points)"
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix C — Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, and project files can be found here"
  }
]