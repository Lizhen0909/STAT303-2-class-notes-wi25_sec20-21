[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science II with python (Class notes)",
    "section": "",
    "text": "Preface\nThese are class notes for the course STAT303-2. This is not the course text-book. You are required to read the relevant sections of the book as mentioned on the course website.\nThe course notes are currently being written, and will continue to being developed as the course progresses (just like the course textbook last quarter). Please report any typos / mistakes / inconsistencies / issues with the class notes / class presentations in your comments here. Thank you!"
  },
  {
    "objectID": "Lec1_SimpleLinearRegression.html",
    "href": "Lec1_SimpleLinearRegression.html",
    "title": "1  Simple Linear Regression",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nDevelop a simple linear regression model that predicts car price based on engine size. Datasets to be used: Car_features_train.csv, Car_prices_train.csv\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990\n    \n  \n\n\n\n\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nols_object = smf.ols(formula = 'price~engineSize', data = train)\n\n\n#Using the fit() function of the 'ols' class to fit the model\nmodel = ols_object.fit()\n\n\n#Printing model summary which contains among other things, the model coefficients\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.390 \n\n\n  Model:                   OLS         Adj. R-squared:        0.390 \n\n\n  Method:             Least Squares    F-statistic:           3177. \n\n\n  Date:             Tue, 27 Dec 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:06:43       Log-Likelihood:      -53949. \n\n\n  No. Observations:        4960        AIC:                1.079e+05\n\n\n  Df Residuals:            4958        BIC:                1.079e+05\n\n\n  Df Model:                   1                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -4122.0357   522.260    -7.893  0.000 -5145.896 -3098.176\n\n\n  engineSize  1.299e+04   230.450    56.361  0.000  1.25e+04  1.34e+04\n\n\n\n\n  Omnibus:       1271.986   Durbin-Watson:         0.517\n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   6490.719\n\n\n  Skew:            1.137    Prob(JB):               0.00\n\n\n  Kurtosis:        8.122    Cond. No.               7.64\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe model equation is: car price = -4122.0357 + 12990 * engineSize\nVisualize the regression line\n\nsns.regplot(x = 'engineSize', y = 'price', data = train, color = 'orange',line_kws={\"color\": \"red\"})\nplt.xlim(-1,7)\n#Note that some of the engineSize values are 0. They are incorrect, and should ideally be imputed before developing the model.\n\n(-1.0, 7.0)\n\n\n\n\n\nPredict the car price for the cars in the test dataset. Datasets to be used: Car_features_test.csv, Car_prices_test.csv\n\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\n\n\n#Using the predict() function associated with the 'model' object to make predictions of car price on test (unknown) data\npred_price = model.predict(testf)#Note that the predict() function finds the predictor 'engineSize' in the testf dataframe, and plugs it's values in the regression equation for prediction.\n\nMake a visualization that compares the predicted car prices with the actual car prices\n\nsns.scatterplot(x = testp.price, y = pred_price)\n#In case of a perfect prediction, all the points must lie on the line x = y.\nsns.lineplot(x = [0,testp.price.max()], y = [0,testp.price.max()],color='orange') #Plotting the line x = y.\nplt.xlabel('Actual price')\nplt.ylabel('Predicted price')\n\nText(0, 0.5, 'Predicted price')\n\n\n\n\n\nThe prediction doesn’t look too good. This is because we are just using one predictor - engine size. We can probably improve the model by adding more predictors when we learn multiple linear regression.\nWhat is the RMSE of the predicted car price?\n\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n12995.1064515487\n\n\nThe root mean squared error in predicting car price is around $13k.\nWhat is the residual standard error based on the training data?\n\nnp.sqrt(model.mse_resid)\n\n12810.109175214136\n\n\nThe residual standard error on the training data is close to the RMSE on the test data. This shows that the performance of the model on unknown data is comparable to its performance on known data. This implies that the model is not overfitting, which is good! In case we overfit a model on the training data, it’s performance on unknonwn data is likely to be worse than that on the training data.\nFind the confidence and prediction intervals of the predicted car price\n\n#Using the get_prediction() function associated with the 'model' object to get the intervals\nintervals = model.get_prediction(testf)\n\n\n#The function requires specifying alpha (probability of Type 1 error) instead of the confidence level to get the intervals\nintervals.summary_frame(alpha=0.05)\n\n\n\n\n\n  \n    \n      \n      mean\n      mean_se\n      mean_ci_lower\n      mean_ci_upper\n      obs_ci_lower\n      obs_ci_upper\n    \n  \n  \n    \n      0\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      1\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      2\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      3\n      8866.245277\n      316.580850\n      8245.606701\n      9486.883853\n      -16254.905974\n      33987.396528\n    \n    \n      4\n      47831.088340\n      468.949360\n      46911.740050\n      48750.436631\n      22700.782946\n      72961.393735\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2667\n      47831.088340\n      468.949360\n      46911.740050\n      48750.436631\n      22700.782946\n      72961.393735\n    \n    \n      2668\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      2669\n      8866.245277\n      316.580850\n      8245.606701\n      9486.883853\n      -16254.905974\n      33987.396528\n    \n    \n      2670\n      21854.526298\n      184.135754\n      21493.538727\n      22215.513869\n      -3261.551421\n      46970.604017\n    \n    \n      2671\n      21854.526298\n      184.135754\n      21493.538727\n      22215.513869\n      -3261.551421\n      46970.604017\n    \n  \n\n2672 rows × 6 columns\n\n\n\nShow the regression line predicting car price based on engine size for test data. Also show the confidence and prediction intervals for the car price.\n\ninterval_table = intervals.summary_frame(alpha=0.05)\n\n\nsns.scatterplot(x = testf.engineSize, y = pred_price,color = 'orange', s = 10)\nsns.lineplot(x = testf.engineSize, y = pred_price, color = 'red')\nsns.lineplot(x = testf.engineSize, y = interval_table.mean_ci_lower, color = 'blue')\nsns.lineplot(x = testf.engineSize, y = interval_table.mean_ci_upper, color = 'blue',label='_nolegend_')\nsns.lineplot(x = testf.engineSize, y = interval_table.obs_ci_lower, color = 'green')\nsns.lineplot(x = testf.engineSize, y = interval_table.obs_ci_upper, color = 'green')\nplt.legend(labels=[\"Regression line\",\"Confidence interval\", \"Prediction interval\"])\n\n<matplotlib.legend.Legend at 0x27c6cfd1070>"
  },
  {
    "objectID": "Lec2_MultipleLinearRegression.html",
    "href": "Lec2_MultipleLinearRegression.html",
    "title": "2  Multiple Linear Regression",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nDevelop a multiple linear regression model that predicts car price based on engine size, year, mileage, and mpg. Datasets to be used: Car_features_train.csv, Car_prices_train.csv\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990\n    \n  \n\n\n\n\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nols_object = smf.ols(formula = 'price~year+mileage+mpg+engineSize', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.660 \n\n\n  Model:                   OLS         Adj. R-squared:        0.660 \n\n\n  Method:             Least Squares    F-statistic:           2410. \n\n\n  Date:             Tue, 27 Dec 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:07:25       Log-Likelihood:      -52497. \n\n\n  No. Observations:        4960        AIC:                1.050e+05\n\n\n  Df Residuals:            4955        BIC:                1.050e+05\n\n\n  Df Model:                   4                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -3.661e+06  1.49e+05   -24.593  0.000 -3.95e+06 -3.37e+06\n\n\n  year        1817.7366    73.751    24.647  0.000  1673.151  1962.322\n\n\n  mileage       -0.1474     0.009   -16.817  0.000    -0.165    -0.130\n\n\n  mpg          -79.3126     9.338    -8.493  0.000   -97.620   -61.006\n\n\n  engineSize  1.218e+04   189.969    64.107  0.000  1.18e+04  1.26e+04\n\n\n\n\n  Omnibus:       2450.973   Durbin-Watson:         0.541 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   31060.548\n\n\n  Skew:            2.045    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.557    Cond. No.           3.83e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nThe model equation is: estimated car price = -3.661e6 + 1818 * year -0.15 * mileage - 79.31 * mpg + 12180 * engineSize\nPredict the car price for the cars in the test dataset. Datasets to be used: Car_features_test.csv, Car_prices_test.csv\n\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\n\n\n#Using the predict() function associated with the 'model' object to make predictions of car price on test (unknown) data\npred_price = model.predict(testf)#Note that the predict() function finds the predictor 'engineSize' in the testf dataframe, and plugs it's values in the regression equation for prediction.\n\nMake a visualization that compares the predicted car prices with the actual car prices\n\nsns.scatterplot(x = testp.price, y = pred_price)\n#In case of a perfect prediction, all the points must lie on the line x = y.\nsns.lineplot(x = [0,testp.price.max()], y = [0,testp.price.max()],color='orange') #Plotting the line x = y.\nplt.xlabel('Actual price')\nplt.ylabel('Predicted price')\n\nText(0, 0.5, 'Predicted price')\n\n\n\n\n\nThe prediction looks better as compared to the one with simple linear regression. This is because we have four predictors to help explain the variation in car price, instead of just one in the case of simple linear regression. Also, all the predictors have a significant relationship with price as evident from their p-values. Thus, all four of them are contributing in explaining the variation. Note the higher values of R2 as compared to the one in the case of simple linear regression.\nWhat is the RMSE of the predicted car price?\n\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n9956.82497993548\n\n\nWhat is the residual standard error based on the training data?\n\nnp.sqrt(model.mse_resid)\n\n9563.74782917604\n\n\n\nsns.scatterplot(x = model.fittedvalues, y=model.resid,color = 'orange')\nsns.lineplot(x = [pred_price.min(),pred_price.max()],y = [0,0],color = 'blue')\nplt.xlabel('Predicted price')\nplt.ylabel('Residual')\n\nText(0, 0.5, 'Residual')\n\n\n\n\n\nWill the explained variation (R-squared) in car price always increase if we add a variable?\nShould we keep on adding variables as long as the explained variation (R-squared) is increasing?\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nnp.random.seed(1)\ntrain['rand_col'] = np.random.rand(train.shape[0])\nols_object = smf.ols(formula = 'price~year+mileage+mpg+engineSize+rand_col', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.661 \n\n\n  Model:                   OLS         Adj. R-squared:        0.660 \n\n\n  Method:             Least Squares    F-statistic:           1928. \n\n\n  Date:             Tue, 27 Dec 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:07:38       Log-Likelihood:      -52497. \n\n\n  No. Observations:        4960        AIC:                1.050e+05\n\n\n  Df Residuals:            4954        BIC:                1.050e+05\n\n\n  Df Model:                   5                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -3.662e+06  1.49e+05   -24.600  0.000 -3.95e+06 -3.37e+06\n\n\n  year        1818.1672    73.753    24.652  0.000  1673.578  1962.756\n\n\n  mileage       -0.1474     0.009   -16.809  0.000    -0.165    -0.130\n\n\n  mpg          -79.2837     9.338    -8.490  0.000   -97.591   -60.976\n\n\n  engineSize  1.218e+04   189.972    64.109  0.000  1.18e+04  1.26e+04\n\n\n  rand_col     451.1226   471.897     0.956  0.339  -474.004  1376.249\n\n\n\n\n  Omnibus:       2451.728   Durbin-Watson:         0.541 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   31040.331\n\n\n  Skew:            2.046    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.552    Cond. No.           3.83e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nAdding a variable with random values to the model (rand_col) increased the explained variation (R-squared). This is because the model has one more parameter to tune to reduce the residual squared error (RSS). However, the p-value of rand_col suggests that its coefficient is zero. Thus, using the model with rand_col may give poorer performance on unknown data, as compared to the model without rand_col. This implies that it is not a good idea to blindly add variables in the model to increase R-squared."
  },
  {
    "objectID": "Section_3312_Variable_Interactions_&_Transformations-Copy1.html#variable-interactions",
    "href": "Section_3312_Variable_Interactions_&_Transformations-Copy1.html#variable-interactions",
    "title": "3  Variable interactions and transformations",
    "section": "3.1 Variable interactions",
    "text": "3.1 Variable interactions\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990"
  },
  {
    "objectID": "Assignment 1.html#instructions",
    "href": "Assignment 1.html#instructions",
    "title": "Appendix A — Assignment A",
    "section": "Instructions",
    "text": "Instructions\n\nYou may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Tuesday, 17th January 2023 at 11:59 pm.\nThere is a bonus question worth 5 points.\nFive points are for properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (1 pt); If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g., printouts of the working directory should not be included in the final submission (1 pt).\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt).\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt).\n\n\nThe maximum possible score in the assigment is 95 + 5 (formatting) + 5 (bonus question) = 105 out of 100. There is no partial credit for the bonus question."
  },
  {
    "objectID": "Assignment 1.html#regression-vs-classification-prediction-vs-inference",
    "href": "Assignment 1.html#regression-vs-classification-prediction-vs-inference",
    "title": "Appendix A — Assignment A",
    "section": "A.1 Regression vs Classification; Prediction vs Inference",
    "text": "A.1 Regression vs Classification; Prediction vs Inference\nExplain (1) whether each scenario is a classification or regression problem, and (2) whether we are most interested in inference or prediction. Answers to both parts must be supported by a justification.\n\nA.1.1 \nConsider a company that is interested in conducting a marketing campaign. The goal is to identify individuals who are likely to respond positively to a marketing campaign, based on observations of demographic variables (such as age, gender, income, etc.) measured on each individual.\n(2+2 points)\n\n\nA.1.2 \nConsider that the company mentioned in the previous question is interested in understanding the impact of advertising promotions in different media types on the company sales. For example, the company is interested in the question, ‘how large of an increase in sales is associated with a given increase in radio vis-a-vis TV advertising?’\n(2+2 points)\n\n\nA.1.3 \nConsider a company selling furniture is interested in the finding the association between demographic characterisitcs of customers (such as age, gender, income, etc.) and their probability of purchase of a particular company product.\n(2+2 points)\n\n\nA.1.4 \nWe are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2022. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.\n(2+2 points)"
  },
  {
    "objectID": "Assignment 1.html#rmse-vs-mae",
    "href": "Assignment 1.html#rmse-vs-mae",
    "title": "Appendix A — Assignment A",
    "section": "A.2 RMSE vs MAE",
    "text": "A.2 RMSE vs MAE\n\nA.2.1 \nDescribe a regression problem, where it will be more appropriate to assess the model accuracy using the root mean squared error (RMSE) metric as compared to the mean absolute error (MAE) metric.\nNote: Don’t use the examples presented in class\n(4 points)\n\n\nA.2.2 \nDescribe a regression problem, where it will be more appropriate to assess the model accuracy using the mean absolute error (MAE) metric as compared to the root mean squared error (RMSE) metric.\nNote: Don’t use the examples presented in class\n(4 points)"
  },
  {
    "objectID": "Assignment 1.html#fnr-vs-fpr",
    "href": "Assignment 1.html#fnr-vs-fpr",
    "title": "Appendix A — Assignment A",
    "section": "A.3 FNR vs FPR",
    "text": "A.3 FNR vs FPR\n\nA.3.1 \nA classification model is developed to predict those customers who will respond positively to a company’s tele-marketing campaign. All those customers that are predicted to respond positively to the campaign will be called by phone to buy the product being marketed. If the customer being called purchases the product (\\(y = 1\\)), the company will get a profit of $100. On the other hand, if they are called and they don’t purchase (\\(y = 0\\)), the company will have a loss of $1. Among FPR (False positive rate) and FNR (False negative rate), which metric is more important to be minimized to reduce the loss associated with misclassification? Justify your answer.\nIn your justification, you must clearly interpret False Negatives (FN) and False Postives (FP) first.\nAssumption: Assume that based on the past marketing campaigns, around 50% of the customers will actually respond positively to the campaign.\n(4 points)\n\n\nA.3.2 \nCan the answer to the previous question change if the assumption stated in the question is false? Justify your answer.\n(6 points)"
  },
  {
    "objectID": "Assignment 1.html#petrol-consumption",
    "href": "Assignment 1.html#petrol-consumption",
    "title": "Appendix A — Assignment A",
    "section": "A.4 Petrol consumption",
    "text": "A.4 Petrol consumption\nRead the dataset petrol_consumption_train.csv. It contains the following five columns:\nPetrol_tax: Petrol tax (cents per gallon)\nPer_capita_income: Average income (dollars)\nPaved_highways: Paved Highways (miles)\nProp_license: Proportion of population with driver’s licenses\nPetrol_consumption: Consumption of petrol (millions of gallons)\n\nA.4.1 \nMake a pairwise plot of all the variables in the dataset. Which variable seems to have the highest linear correlation with Petrol_consumption? Let this variable be predictor P. Note: If you cannot figure out P by looking at the visualization, you may find the pairwise linear correlation coefficient to identify P.\n(4 points)\n\n\nA.4.2 \nFit a simple linear regression model to predict Petrol_consumption based on predictor P (identified in the previous part). Print the model summary.\n(4 points)\n\n\nA.4.3 \nInterpret the coefficient of Prop_license. What is the increase in petrol consumption for an increase of 0.05 in P?\n(2+2 points)\n\n\nA.4.4 \nDoes petrol consumption have a statistically significant relationship with the predictor P? Justify your answer.\n(4 points)\n\n\nA.4.5 \nWhat is the R-squared? Interpret its value.\n(4 points)\n\n\nA.4.6 \nUse the model developed above to estimate the petrol consumption for a state in which 50% of the population has a driver’s license. What are the confidence and prediction intervals for your estimate? Which interval includes the irreducible error?\n(4+3+3+2 = 12 points)\n\n\nA.4.7 \nUse the model developed above to estimate the petrol consumption for a state in which 10% of the population has a driver’s license. Are you getting a reasonable estimate? Why or why not?\n(5 points)\n\n\nA.4.8 \nWhat is the residual standard error of the model?\n(4 points)\n\n\nA.4.9 \nUsing the model developed above, predict the petrol consumption for the observations in petrol_consumption_test.csv. Find the RMSE (Root mean squared error). Include the units of RMSE in your answer.\n(5 points)\n\n\nA.4.10 \nBased on the answers to the previous two questions, do you think the model is overfitting? Justify your answer.\n(4 points)\nMake a scatterplot of Petrol_consumption vs Prop_license using petrol_consumption_test.csv. Over the scatterplot, plot the regression line, the prediction interval, and the confidence interval. Distinguish the regression line, prediction interval lines, and confidence interval lines with the following colors. Include the legend as well.\n\nRegression line: red\nConfidence interval lines: blue\nPrediction interval lines: green\n\n(4 points)\nAmong the confidence and prediction intervals, which interval is wider, and why?\n(1+2 points)\n\n\nA.4.11 \nFind the correlation between Petrol_consumption and the rest of the variables in petrol_consumption_train.csv. Based on the correlations, a simple linear regression model with which predictor will have the least R-squared value for predicting Petrol_consumption. Don’t develop any linear regression models.\n(4 points)\nBonus point question\n(5 points - no partial credit)\n\n\nA.4.12 \nFit a simple linear regression model to predict Petrol_consumption based on predictor P, but without an intercept term.\n(you must answer this correctly to qualify for earning bonus points)\n\n\nA.4.13 \nEstimate the petrol consumption for the observations in petrol_consumption_test.csv using the model in developed in the previous question. Find the RMSE.\n(you must answer this correctly to qualify for earning bonus points)\n\n\nA.4.14 \nThe RMSE for the models with and without the intercept are similar, which indicates that both models are almost equally good. However, the R-squared for the model without intercept is much higher than the R-squared for the model with the intercept. Why? Justify your answer.\n(5 points)"
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix B — Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, and project files can be found here"
  }
]